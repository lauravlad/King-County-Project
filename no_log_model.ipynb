{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>price</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>view_1</th>\n",
       "      <th>view_2</th>\n",
       "      <th>...</th>\n",
       "      <th>zip_98146</th>\n",
       "      <th>zip_98148</th>\n",
       "      <th>zip_98155</th>\n",
       "      <th>zip_98166</th>\n",
       "      <th>zip_98168</th>\n",
       "      <th>zip_98177</th>\n",
       "      <th>zip_98178</th>\n",
       "      <th>zip_98188</th>\n",
       "      <th>zip_98198</th>\n",
       "      <th>zip_98199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>7242.0</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>7639.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>8062.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>7503.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     price  sqft_living  sqft_lot  sqft_above  sqft_living15  \\\n",
       "0           0  221900.0       1180.0    5650.0      1180.0         1340.0   \n",
       "1           1  538000.0       2570.0    7242.0      2170.0         1690.0   \n",
       "2           2  180000.0        770.0   10000.0       770.0         2720.0   \n",
       "3           3  604000.0       1960.0    5000.0      1050.0         1360.0   \n",
       "4           4  510000.0       1680.0    8080.0      1680.0         1800.0   \n",
       "\n",
       "   sqft_lot15  bathrooms  view_1  view_2  ...  zip_98146  zip_98148  \\\n",
       "0      5650.0       1.00       0       0  ...          0          0   \n",
       "1      7639.0       2.25       0       0  ...          0          0   \n",
       "2      8062.0       1.00       0       0  ...          0          0   \n",
       "3      5000.0       3.00       0       0  ...          0          0   \n",
       "4      7503.0       2.00       0       0  ...          0          0   \n",
       "\n",
       "   zip_98155  zip_98166  zip_98168  zip_98177  zip_98178  zip_98188  \\\n",
       "0          0          0          0          0          1          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98198  zip_98199  \n",
       "0          0          0  \n",
       "1          0          0  \n",
       "2          0          0  \n",
       "3          0          0  \n",
       "4          0          0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import already cleaned data and verify its columns with .head.\n",
    "data = pd.read_csv('data2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'price',\n",
       " 'sqft_living',\n",
       " 'sqft_lot',\n",
       " 'sqft_above',\n",
       " 'sqft_living15',\n",
       " 'sqft_lot15',\n",
       " 'bathrooms',\n",
       " 'view_1',\n",
       " 'view_2',\n",
       " 'view_3',\n",
       " 'view_4',\n",
       " 'condition_3',\n",
       " 'condition_4',\n",
       " 'condition_5',\n",
       " 'floor_1',\n",
       " 'floor_1_5',\n",
       " 'floor_2',\n",
       " 'floor_2_5',\n",
       " 'renovated',\n",
       " 'waterfront',\n",
       " 'gr_5',\n",
       " 'gr_6',\n",
       " 'gr_7',\n",
       " 'gr_8',\n",
       " 'gr_9',\n",
       " 'gr_10',\n",
       " 'basement',\n",
       " 'bedroom_1',\n",
       " 'bedroom_2',\n",
       " 'bedroom_3',\n",
       " 'bedroom_4',\n",
       " 'bedroom_5',\n",
       " 'bedroom_6',\n",
       " 'zip_98001',\n",
       " 'zip_98002',\n",
       " 'zip_98003',\n",
       " 'zip_98004',\n",
       " 'zip_98005',\n",
       " 'zip_98006',\n",
       " 'zip_98007',\n",
       " 'zip_98008',\n",
       " 'zip_98010',\n",
       " 'zip_98011',\n",
       " 'zip_98014',\n",
       " 'zip_98019',\n",
       " 'zip_98022',\n",
       " 'zip_98023',\n",
       " 'zip_98024',\n",
       " 'zip_98027',\n",
       " 'zip_98028',\n",
       " 'zip_98029',\n",
       " 'zip_98030',\n",
       " 'zip_98031',\n",
       " 'zip_98032',\n",
       " 'zip_98033',\n",
       " 'zip_98034',\n",
       " 'zip_98038',\n",
       " 'zip_98039',\n",
       " 'zip_98040',\n",
       " 'zip_98042',\n",
       " 'zip_98045',\n",
       " 'zip_98052',\n",
       " 'zip_98053',\n",
       " 'zip_98055',\n",
       " 'zip_98056',\n",
       " 'zip_98058',\n",
       " 'zip_98059',\n",
       " 'zip_98065',\n",
       " 'zip_98070',\n",
       " 'zip_98072',\n",
       " 'zip_98074',\n",
       " 'zip_98075',\n",
       " 'zip_98077',\n",
       " 'zip_98092',\n",
       " 'zip_98102',\n",
       " 'zip_98103',\n",
       " 'zip_98105',\n",
       " 'zip_98106',\n",
       " 'zip_98107',\n",
       " 'zip_98108',\n",
       " 'zip_98109',\n",
       " 'zip_98112',\n",
       " 'zip_98115',\n",
       " 'zip_98116',\n",
       " 'zip_98117',\n",
       " 'zip_98118',\n",
       " 'zip_98119',\n",
       " 'zip_98122',\n",
       " 'zip_98125',\n",
       " 'zip_98126',\n",
       " 'zip_98133',\n",
       " 'zip_98136',\n",
       " 'zip_98144',\n",
       " 'zip_98146',\n",
       " 'zip_98148',\n",
       " 'zip_98155',\n",
       " 'zip_98166',\n",
       " 'zip_98168',\n",
       " 'zip_98177',\n",
       " 'zip_98178',\n",
       " 'zip_98188',\n",
       " 'zip_98198',\n",
       " 'zip_98199']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a list with all the columns.\n",
    "data.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1st linear model with Price as the target variable in statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance are located at the top table - R-square and adjusted R-squared.\n",
    "The model significance information is located at the top table - F-statistics and p-Value\n",
    "The explanatory of each variables information is located in the middle table - from bedrooms to zipcode.\n",
    "The model bias information is located at the bottom of the table - Jarque-Bera (JB) and Durbin-Watson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Adjusted R-Squared value of 0.65 would indicate that the current model (the explanatory variables modeled using linear regression) explains approximately 65 percent of the variation in the dependent variable. Said another way, the current model explain approximately 65 percent of the house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('price', axis=1)\n",
    "y = data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.835</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.835</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   956.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 20 Jun 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:45:01</td>     <th>  Log-Likelihood:    </th> <td>-2.4099e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 18735</td>      <th>  AIC:               </th>  <td>4.822e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 18635</td>      <th>  BIC:               </th>  <td>4.830e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    99</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td> 1.034e+05</td> <td> 6218.849</td> <td>   16.623</td> <td> 0.000</td> <td> 9.12e+04</td> <td> 1.16e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Unnamed: 0</th>    <td>    0.0451</td> <td>    0.114</td> <td>    0.396</td> <td> 0.692</td> <td>   -0.178</td> <td>    0.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th>   <td>   75.5299</td> <td>    3.508</td> <td>   21.531</td> <td> 0.000</td> <td>   68.654</td> <td>   82.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>      <td>    1.1412</td> <td>    0.114</td> <td>   10.023</td> <td> 0.000</td> <td>    0.918</td> <td>    1.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_above</th>    <td>   42.7475</td> <td>    3.986</td> <td>   10.723</td> <td> 0.000</td> <td>   34.934</td> <td>   50.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15</th> <td>   26.8813</td> <td>    2.068</td> <td>   12.996</td> <td> 0.000</td> <td>   22.827</td> <td>   30.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot15</th>    <td>   -0.7035</td> <td>    0.169</td> <td>   -4.151</td> <td> 0.000</td> <td>   -1.036</td> <td>   -0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>     <td> 1.352e+04</td> <td> 1734.057</td> <td>    7.797</td> <td> 0.000</td> <td> 1.01e+04</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_1</th>        <td> 6.944e+04</td> <td> 5861.190</td> <td>   11.848</td> <td> 0.000</td> <td>  5.8e+04</td> <td> 8.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_2</th>        <td> 7.051e+04</td> <td> 3701.810</td> <td>   19.048</td> <td> 0.000</td> <td> 6.33e+04</td> <td> 7.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_3</th>        <td> 1.294e+05</td> <td> 5435.619</td> <td>   23.809</td> <td> 0.000</td> <td> 1.19e+05</td> <td>  1.4e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_4</th>        <td> 2.261e+05</td> <td> 8748.938</td> <td>   25.840</td> <td> 0.000</td> <td> 2.09e+05</td> <td> 2.43e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>   <td> 1.987e+04</td> <td> 7923.209</td> <td>    2.507</td> <td> 0.012</td> <td> 4336.265</td> <td> 3.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>   <td> 4.359e+04</td> <td> 7971.258</td> <td>    5.468</td> <td> 0.000</td> <td>  2.8e+04</td> <td> 5.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>   <td> 8.336e+04</td> <td> 8239.293</td> <td>   10.118</td> <td> 0.000</td> <td> 6.72e+04</td> <td> 9.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_1</th>       <td> 3.178e+04</td> <td> 2691.812</td> <td>   11.807</td> <td> 0.000</td> <td> 2.65e+04</td> <td> 3.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_1_5</th>     <td> 3.778e+04</td> <td> 3100.950</td> <td>   12.183</td> <td> 0.000</td> <td> 3.17e+04</td> <td> 4.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_2</th>       <td> 1.329e+04</td> <td> 2880.166</td> <td>    4.613</td> <td> 0.000</td> <td> 7640.335</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_2_5</th>     <td> 2.053e+04</td> <td> 7448.952</td> <td>    2.756</td> <td> 0.006</td> <td> 5927.597</td> <td> 3.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>renovated</th>     <td> 5.477e+04</td> <td> 3946.169</td> <td>   13.878</td> <td> 0.000</td> <td>  4.7e+04</td> <td> 6.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>    <td> 2.017e+05</td> <td> 1.49e+04</td> <td>   13.508</td> <td> 0.000</td> <td> 1.72e+05</td> <td> 2.31e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_5</th>          <td>-5.221e+04</td> <td> 5850.324</td> <td>   -8.925</td> <td> 0.000</td> <td>-6.37e+04</td> <td>-4.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_6</th>          <td>-4.879e+04</td> <td> 2624.362</td> <td>  -18.589</td> <td> 0.000</td> <td>-5.39e+04</td> <td>-4.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_7</th>          <td>-3.489e+04</td> <td> 1902.074</td> <td>  -18.341</td> <td> 0.000</td> <td>-3.86e+04</td> <td>-3.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_8</th>          <td> 4399.2134</td> <td> 2137.891</td> <td>    2.058</td> <td> 0.040</td> <td>  208.753</td> <td> 8589.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_9</th>          <td> 8.243e+04</td> <td> 2989.938</td> <td>   27.568</td> <td> 0.000</td> <td> 7.66e+04</td> <td> 8.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_10</th>         <td> 1.524e+05</td> <td> 4368.222</td> <td>   34.896</td> <td> 0.000</td> <td> 1.44e+05</td> <td> 1.61e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>basement</th>      <td> 2943.4008</td> <td> 2809.329</td> <td>    1.048</td> <td> 0.295</td> <td>-2563.141</td> <td> 8449.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_1</th>     <td> 9682.0173</td> <td> 6641.048</td> <td>    1.458</td> <td> 0.145</td> <td>-3335.044</td> <td> 2.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_2</th>     <td> 1.906e+04</td> <td> 2601.134</td> <td>    7.326</td> <td> 0.000</td> <td>  1.4e+04</td> <td> 2.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_3</th>     <td> 2.804e+04</td> <td> 2126.942</td> <td>   13.185</td> <td> 0.000</td> <td> 2.39e+04</td> <td> 3.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_4</th>     <td> 2.419e+04</td> <td> 2354.289</td> <td>   10.273</td> <td> 0.000</td> <td> 1.96e+04</td> <td> 2.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_5</th>     <td> 1.478e+04</td> <td> 3254.560</td> <td>    4.540</td> <td> 0.000</td> <td> 8396.595</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_6</th>     <td> 7629.7978</td> <td> 6270.965</td> <td>    1.217</td> <td> 0.224</td> <td>-4661.866</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98001</th>     <td>-1.927e+05</td> <td> 5077.699</td> <td>  -37.943</td> <td> 0.000</td> <td>-2.03e+05</td> <td>-1.83e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98002</th>     <td>-1.811e+05</td> <td> 6746.142</td> <td>  -26.841</td> <td> 0.000</td> <td>-1.94e+05</td> <td>-1.68e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98003</th>     <td>-1.894e+05</td> <td> 5750.400</td> <td>  -32.945</td> <td> 0.000</td> <td>-2.01e+05</td> <td>-1.78e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98004</th>     <td> 4.016e+05</td> <td> 6775.439</td> <td>   59.275</td> <td> 0.000</td> <td> 3.88e+05</td> <td> 4.15e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98005</th>     <td>  1.45e+05</td> <td> 7742.264</td> <td>   18.727</td> <td> 0.000</td> <td>  1.3e+05</td> <td>  1.6e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98006</th>     <td> 8.358e+04</td> <td> 4865.391</td> <td>   17.178</td> <td> 0.000</td> <td>  7.4e+04</td> <td> 9.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98007</th>     <td>  6.86e+04</td> <td> 8125.612</td> <td>    8.442</td> <td> 0.000</td> <td> 5.27e+04</td> <td> 8.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98008</th>     <td> 5.862e+04</td> <td> 5806.941</td> <td>   10.095</td> <td> 0.000</td> <td> 4.72e+04</td> <td>    7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98010</th>     <td> -1.16e+05</td> <td> 1.08e+04</td> <td>  -10.764</td> <td> 0.000</td> <td>-1.37e+05</td> <td>-9.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98011</th>     <td>-4.959e+04</td> <td> 6877.175</td> <td>   -7.211</td> <td> 0.000</td> <td>-6.31e+04</td> <td>-3.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98014</th>     <td>-7.799e+04</td> <td> 1.09e+04</td> <td>   -7.188</td> <td> 0.000</td> <td>-9.93e+04</td> <td>-5.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98019</th>     <td>-9.612e+04</td> <td> 7482.866</td> <td>  -12.845</td> <td> 0.000</td> <td>-1.11e+05</td> <td>-8.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98022</th>     <td>-1.987e+05</td> <td> 7102.561</td> <td>  -27.981</td> <td> 0.000</td> <td>-2.13e+05</td> <td>-1.85e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98023</th>     <td>-2.107e+05</td> <td> 4360.035</td> <td>  -48.321</td> <td> 0.000</td> <td>-2.19e+05</td> <td>-2.02e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98024</th>     <td> -5.06e+04</td> <td> 1.49e+04</td> <td>   -3.396</td> <td> 0.001</td> <td>-7.98e+04</td> <td>-2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98027</th>     <td> 5869.4507</td> <td> 5546.295</td> <td>    1.058</td> <td> 0.290</td> <td>-5001.794</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98028</th>     <td>-5.481e+04</td> <td> 5687.746</td> <td>   -9.637</td> <td> 0.000</td> <td> -6.6e+04</td> <td>-4.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98029</th>     <td>  3.24e+04</td> <td> 5599.378</td> <td>    5.785</td> <td> 0.000</td> <td> 2.14e+04</td> <td> 4.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98030</th>     <td>-1.852e+05</td> <td> 5990.973</td> <td>  -30.907</td> <td> 0.000</td> <td>-1.97e+05</td> <td>-1.73e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98031</th>     <td>-1.755e+05</td> <td> 5822.182</td> <td>  -30.148</td> <td> 0.000</td> <td>-1.87e+05</td> <td>-1.64e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98032</th>     <td>-1.891e+05</td> <td> 8457.939</td> <td>  -22.360</td> <td> 0.000</td> <td>-2.06e+05</td> <td>-1.73e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98033</th>     <td> 1.475e+05</td> <td> 4857.597</td> <td>   30.367</td> <td> 0.000</td> <td> 1.38e+05</td> <td> 1.57e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98034</th>     <td>  178.3698</td> <td> 4228.666</td> <td>    0.042</td> <td> 0.966</td> <td>-8110.202</td> <td> 8466.941</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98038</th>     <td>-1.579e+05</td> <td> 4286.491</td> <td>  -36.830</td> <td> 0.000</td> <td>-1.66e+05</td> <td>-1.49e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98039</th>     <td> 5.804e+05</td> <td> 2.47e+04</td> <td>   23.469</td> <td> 0.000</td> <td> 5.32e+05</td> <td> 6.29e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98040</th>     <td> 2.694e+05</td> <td> 6861.514</td> <td>   39.264</td> <td> 0.000</td> <td> 2.56e+05</td> <td> 2.83e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98042</th>     <td>-1.871e+05</td> <td> 4230.393</td> <td>  -44.227</td> <td> 0.000</td> <td>-1.95e+05</td> <td>-1.79e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98045</th>     <td>-8.448e+04</td> <td> 7031.596</td> <td>  -12.015</td> <td> 0.000</td> <td>-9.83e+04</td> <td>-7.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98052</th>     <td> 6.263e+04</td> <td> 4188.238</td> <td>   14.953</td> <td> 0.000</td> <td> 5.44e+04</td> <td> 7.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98053</th>     <td> 4.595e+04</td> <td> 5525.597</td> <td>    8.316</td> <td> 0.000</td> <td> 3.51e+04</td> <td> 5.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98055</th>     <td>-1.481e+05</td> <td> 5861.945</td> <td>  -25.263</td> <td> 0.000</td> <td> -1.6e+05</td> <td>-1.37e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98056</th>     <td>-9.719e+04</td> <td> 4833.078</td> <td>  -20.109</td> <td> 0.000</td> <td>-1.07e+05</td> <td>-8.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98058</th>     <td>-1.553e+05</td> <td> 4591.973</td> <td>  -33.815</td> <td> 0.000</td> <td>-1.64e+05</td> <td>-1.46e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98059</th>     <td>-1.007e+05</td> <td> 4745.590</td> <td>  -21.219</td> <td> 0.000</td> <td> -1.1e+05</td> <td>-9.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98065</th>     <td>-6.833e+04</td> <td> 5940.374</td> <td>  -11.503</td> <td> 0.000</td> <td>   -8e+04</td> <td>-5.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98070</th>     <td>-1.392e+05</td> <td> 1.21e+04</td> <td>  -11.505</td> <td> 0.000</td> <td>-1.63e+05</td> <td>-1.15e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98072</th>     <td>-2.636e+04</td> <td> 6249.537</td> <td>   -4.218</td> <td> 0.000</td> <td>-3.86e+04</td> <td>-1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98074</th>     <td> 1.142e+04</td> <td> 4980.010</td> <td>    2.293</td> <td> 0.022</td> <td> 1656.967</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98075</th>     <td> 2.205e+04</td> <td> 5905.198</td> <td>    3.734</td> <td> 0.000</td> <td> 1.05e+04</td> <td> 3.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98077</th>     <td>-4.219e+04</td> <td> 8498.051</td> <td>   -4.964</td> <td> 0.000</td> <td>-5.88e+04</td> <td>-2.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98092</th>     <td>-2.148e+05</td> <td> 5530.053</td> <td>  -38.834</td> <td> 0.000</td> <td>-2.26e+05</td> <td>-2.04e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98102</th>     <td> 2.547e+05</td> <td> 1.06e+04</td> <td>   24.013</td> <td> 0.000</td> <td> 2.34e+05</td> <td> 2.75e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98103</th>     <td>  1.79e+05</td> <td> 4704.836</td> <td>   38.037</td> <td> 0.000</td> <td>  1.7e+05</td> <td> 1.88e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98105</th>     <td>  2.56e+05</td> <td> 6863.925</td> <td>   37.292</td> <td> 0.000</td> <td> 2.43e+05</td> <td> 2.69e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98106</th>     <td> -6.46e+04</td> <td> 5377.450</td> <td>  -12.014</td> <td> 0.000</td> <td>-7.51e+04</td> <td>-5.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98107</th>     <td> 1.484e+05</td> <td> 6784.772</td> <td>   21.871</td> <td> 0.000</td> <td> 1.35e+05</td> <td> 1.62e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98108</th>     <td>-6.421e+04</td> <td> 6983.389</td> <td>   -9.194</td> <td> 0.000</td> <td>-7.79e+04</td> <td>-5.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98109</th>     <td> 2.751e+05</td> <td>    1e+04</td> <td>   27.494</td> <td> 0.000</td> <td> 2.56e+05</td> <td> 2.95e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98112</th>     <td> 3.196e+05</td> <td> 6876.538</td> <td>   46.476</td> <td> 0.000</td> <td> 3.06e+05</td> <td> 3.33e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98115</th>     <td> 1.538e+05</td> <td> 4230.899</td> <td>   36.363</td> <td> 0.000</td> <td> 1.46e+05</td> <td> 1.62e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98116</th>     <td> 1.183e+05</td> <td> 5611.307</td> <td>   21.091</td> <td> 0.000</td> <td> 1.07e+05</td> <td> 1.29e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98117</th>     <td> 1.429e+05</td> <td> 4387.114</td> <td>   32.573</td> <td> 0.000</td> <td> 1.34e+05</td> <td> 1.52e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98118</th>     <td>-1.747e+04</td> <td> 4383.926</td> <td>   -3.985</td> <td> 0.000</td> <td>-2.61e+04</td> <td>-8876.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98119</th>     <td> 2.659e+05</td> <td> 7878.957</td> <td>   33.748</td> <td> 0.000</td> <td>  2.5e+05</td> <td> 2.81e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98122</th>     <td> 1.418e+05</td> <td> 6020.136</td> <td>   23.562</td> <td> 0.000</td> <td>  1.3e+05</td> <td> 1.54e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98125</th>     <td> 2.153e+04</td> <td> 4998.849</td> <td>    4.308</td> <td> 0.000</td> <td> 1.17e+04</td> <td> 3.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98126</th>     <td> 9027.8145</td> <td> 5241.439</td> <td>    1.722</td> <td> 0.085</td> <td>-1245.883</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98133</th>     <td>-2.648e+04</td> <td> 4534.051</td> <td>   -5.840</td> <td> 0.000</td> <td>-3.54e+04</td> <td>-1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98136</th>     <td> 6.978e+04</td> <td> 6182.895</td> <td>   11.286</td> <td> 0.000</td> <td> 5.77e+04</td> <td> 8.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98144</th>     <td> 7.094e+04</td> <td> 5473.044</td> <td>   12.962</td> <td> 0.000</td> <td> 6.02e+04</td> <td> 8.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98146</th>     <td>-7.699e+04</td> <td> 5708.785</td> <td>  -13.485</td> <td> 0.000</td> <td>-8.82e+04</td> <td>-6.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98148</th>     <td>-1.259e+05</td> <td> 1.29e+04</td> <td>   -9.786</td> <td> 0.000</td> <td>-1.51e+05</td> <td>-1.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98155</th>     <td>-4.319e+04</td> <td> 4590.220</td> <td>   -9.409</td> <td> 0.000</td> <td>-5.22e+04</td> <td>-3.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98166</th>     <td>-9.094e+04</td> <td> 6085.939</td> <td>  -14.943</td> <td> 0.000</td> <td>-1.03e+05</td> <td> -7.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98168</th>     <td>-1.355e+05</td> <td> 5911.938</td> <td>  -22.924</td> <td> 0.000</td> <td>-1.47e+05</td> <td>-1.24e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98177</th>     <td>  4.16e+04</td> <td> 6147.864</td> <td>    6.766</td> <td> 0.000</td> <td> 2.95e+04</td> <td> 5.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98178</th>     <td>-1.384e+05</td> <td> 5942.182</td> <td>  -23.283</td> <td> 0.000</td> <td> -1.5e+05</td> <td>-1.27e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98188</th>     <td>-1.506e+05</td> <td> 8267.916</td> <td>  -18.219</td> <td> 0.000</td> <td>-1.67e+05</td> <td>-1.34e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98198</th>     <td>-1.718e+05</td> <td> 5775.203</td> <td>  -29.744</td> <td> 0.000</td> <td>-1.83e+05</td> <td> -1.6e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98199</th>     <td> 1.947e+05</td> <td> 5665.782</td> <td>   34.366</td> <td> 0.000</td> <td> 1.84e+05</td> <td> 2.06e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3571.303</td> <th>  Durbin-Watson:     </th> <td>   1.981</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>19833.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.806</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.776</td>  <th>  Cond. No.          </th> <td>6.47e+19</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.8e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.835\n",
       "Model:                            OLS   Adj. R-squared:                  0.835\n",
       "Method:                 Least Squares   F-statistic:                     956.0\n",
       "Date:                Sat, 20 Jun 2020   Prob (F-statistic):               0.00\n",
       "Time:                        19:45:01   Log-Likelihood:            -2.4099e+05\n",
       "No. Observations:               18735   AIC:                         4.822e+05\n",
       "Df Residuals:                   18635   BIC:                         4.830e+05\n",
       "Df Model:                          99                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const          1.034e+05   6218.849     16.623      0.000    9.12e+04    1.16e+05\n",
       "Unnamed: 0        0.0451      0.114      0.396      0.692      -0.178       0.268\n",
       "sqft_living      75.5299      3.508     21.531      0.000      68.654      82.406\n",
       "sqft_lot          1.1412      0.114     10.023      0.000       0.918       1.364\n",
       "sqft_above       42.7475      3.986     10.723      0.000      34.934      50.561\n",
       "sqft_living15    26.8813      2.068     12.996      0.000      22.827      30.936\n",
       "sqft_lot15       -0.7035      0.169     -4.151      0.000      -1.036      -0.371\n",
       "bathrooms      1.352e+04   1734.057      7.797      0.000    1.01e+04    1.69e+04\n",
       "view_1         6.944e+04   5861.190     11.848      0.000     5.8e+04    8.09e+04\n",
       "view_2         7.051e+04   3701.810     19.048      0.000    6.33e+04    7.78e+04\n",
       "view_3         1.294e+05   5435.619     23.809      0.000    1.19e+05     1.4e+05\n",
       "view_4         2.261e+05   8748.938     25.840      0.000    2.09e+05    2.43e+05\n",
       "condition_3    1.987e+04   7923.209      2.507      0.012    4336.265    3.54e+04\n",
       "condition_4    4.359e+04   7971.258      5.468      0.000     2.8e+04    5.92e+04\n",
       "condition_5    8.336e+04   8239.293     10.118      0.000    6.72e+04    9.95e+04\n",
       "floor_1        3.178e+04   2691.812     11.807      0.000    2.65e+04    3.71e+04\n",
       "floor_1_5      3.778e+04   3100.950     12.183      0.000    3.17e+04    4.39e+04\n",
       "floor_2        1.329e+04   2880.166      4.613      0.000    7640.335    1.89e+04\n",
       "floor_2_5      2.053e+04   7448.952      2.756      0.006    5927.597    3.51e+04\n",
       "renovated      5.477e+04   3946.169     13.878      0.000     4.7e+04    6.25e+04\n",
       "waterfront     2.017e+05   1.49e+04     13.508      0.000    1.72e+05    2.31e+05\n",
       "gr_5          -5.221e+04   5850.324     -8.925      0.000   -6.37e+04   -4.07e+04\n",
       "gr_6          -4.879e+04   2624.362    -18.589      0.000   -5.39e+04   -4.36e+04\n",
       "gr_7          -3.489e+04   1902.074    -18.341      0.000   -3.86e+04   -3.12e+04\n",
       "gr_8           4399.2134   2137.891      2.058      0.040     208.753    8589.674\n",
       "gr_9           8.243e+04   2989.938     27.568      0.000    7.66e+04    8.83e+04\n",
       "gr_10          1.524e+05   4368.222     34.896      0.000    1.44e+05    1.61e+05\n",
       "basement       2943.4008   2809.329      1.048      0.295   -2563.141    8449.943\n",
       "bedroom_1      9682.0173   6641.048      1.458      0.145   -3335.044    2.27e+04\n",
       "bedroom_2      1.906e+04   2601.134      7.326      0.000     1.4e+04    2.42e+04\n",
       "bedroom_3      2.804e+04   2126.942     13.185      0.000    2.39e+04    3.22e+04\n",
       "bedroom_4      2.419e+04   2354.289     10.273      0.000    1.96e+04    2.88e+04\n",
       "bedroom_5      1.478e+04   3254.560      4.540      0.000    8396.595    2.12e+04\n",
       "bedroom_6      7629.7978   6270.965      1.217      0.224   -4661.866    1.99e+04\n",
       "zip_98001     -1.927e+05   5077.699    -37.943      0.000   -2.03e+05   -1.83e+05\n",
       "zip_98002     -1.811e+05   6746.142    -26.841      0.000   -1.94e+05   -1.68e+05\n",
       "zip_98003     -1.894e+05   5750.400    -32.945      0.000   -2.01e+05   -1.78e+05\n",
       "zip_98004      4.016e+05   6775.439     59.275      0.000    3.88e+05    4.15e+05\n",
       "zip_98005       1.45e+05   7742.264     18.727      0.000     1.3e+05     1.6e+05\n",
       "zip_98006      8.358e+04   4865.391     17.178      0.000     7.4e+04    9.31e+04\n",
       "zip_98007       6.86e+04   8125.612      8.442      0.000    5.27e+04    8.45e+04\n",
       "zip_98008      5.862e+04   5806.941     10.095      0.000    4.72e+04       7e+04\n",
       "zip_98010      -1.16e+05   1.08e+04    -10.764      0.000   -1.37e+05   -9.49e+04\n",
       "zip_98011     -4.959e+04   6877.175     -7.211      0.000   -6.31e+04   -3.61e+04\n",
       "zip_98014     -7.799e+04   1.09e+04     -7.188      0.000   -9.93e+04   -5.67e+04\n",
       "zip_98019     -9.612e+04   7482.866    -12.845      0.000   -1.11e+05   -8.14e+04\n",
       "zip_98022     -1.987e+05   7102.561    -27.981      0.000   -2.13e+05   -1.85e+05\n",
       "zip_98023     -2.107e+05   4360.035    -48.321      0.000   -2.19e+05   -2.02e+05\n",
       "zip_98024      -5.06e+04   1.49e+04     -3.396      0.001   -7.98e+04   -2.14e+04\n",
       "zip_98027      5869.4507   5546.295      1.058      0.290   -5001.794    1.67e+04\n",
       "zip_98028     -5.481e+04   5687.746     -9.637      0.000    -6.6e+04   -4.37e+04\n",
       "zip_98029       3.24e+04   5599.378      5.785      0.000    2.14e+04    4.34e+04\n",
       "zip_98030     -1.852e+05   5990.973    -30.907      0.000   -1.97e+05   -1.73e+05\n",
       "zip_98031     -1.755e+05   5822.182    -30.148      0.000   -1.87e+05   -1.64e+05\n",
       "zip_98032     -1.891e+05   8457.939    -22.360      0.000   -2.06e+05   -1.73e+05\n",
       "zip_98033      1.475e+05   4857.597     30.367      0.000    1.38e+05    1.57e+05\n",
       "zip_98034       178.3698   4228.666      0.042      0.966   -8110.202    8466.941\n",
       "zip_98038     -1.579e+05   4286.491    -36.830      0.000   -1.66e+05   -1.49e+05\n",
       "zip_98039      5.804e+05   2.47e+04     23.469      0.000    5.32e+05    6.29e+05\n",
       "zip_98040      2.694e+05   6861.514     39.264      0.000    2.56e+05    2.83e+05\n",
       "zip_98042     -1.871e+05   4230.393    -44.227      0.000   -1.95e+05   -1.79e+05\n",
       "zip_98045     -8.448e+04   7031.596    -12.015      0.000   -9.83e+04   -7.07e+04\n",
       "zip_98052      6.263e+04   4188.238     14.953      0.000    5.44e+04    7.08e+04\n",
       "zip_98053      4.595e+04   5525.597      8.316      0.000    3.51e+04    5.68e+04\n",
       "zip_98055     -1.481e+05   5861.945    -25.263      0.000    -1.6e+05   -1.37e+05\n",
       "zip_98056     -9.719e+04   4833.078    -20.109      0.000   -1.07e+05   -8.77e+04\n",
       "zip_98058     -1.553e+05   4591.973    -33.815      0.000   -1.64e+05   -1.46e+05\n",
       "zip_98059     -1.007e+05   4745.590    -21.219      0.000    -1.1e+05   -9.14e+04\n",
       "zip_98065     -6.833e+04   5940.374    -11.503      0.000      -8e+04   -5.67e+04\n",
       "zip_98070     -1.392e+05   1.21e+04    -11.505      0.000   -1.63e+05   -1.15e+05\n",
       "zip_98072     -2.636e+04   6249.537     -4.218      0.000   -3.86e+04   -1.41e+04\n",
       "zip_98074      1.142e+04   4980.010      2.293      0.022    1656.967    2.12e+04\n",
       "zip_98075      2.205e+04   5905.198      3.734      0.000    1.05e+04    3.36e+04\n",
       "zip_98077     -4.219e+04   8498.051     -4.964      0.000   -5.88e+04   -2.55e+04\n",
       "zip_98092     -2.148e+05   5530.053    -38.834      0.000   -2.26e+05   -2.04e+05\n",
       "zip_98102      2.547e+05   1.06e+04     24.013      0.000    2.34e+05    2.75e+05\n",
       "zip_98103       1.79e+05   4704.836     38.037      0.000     1.7e+05    1.88e+05\n",
       "zip_98105       2.56e+05   6863.925     37.292      0.000    2.43e+05    2.69e+05\n",
       "zip_98106      -6.46e+04   5377.450    -12.014      0.000   -7.51e+04   -5.41e+04\n",
       "zip_98107      1.484e+05   6784.772     21.871      0.000    1.35e+05    1.62e+05\n",
       "zip_98108     -6.421e+04   6983.389     -9.194      0.000   -7.79e+04   -5.05e+04\n",
       "zip_98109      2.751e+05      1e+04     27.494      0.000    2.56e+05    2.95e+05\n",
       "zip_98112      3.196e+05   6876.538     46.476      0.000    3.06e+05    3.33e+05\n",
       "zip_98115      1.538e+05   4230.899     36.363      0.000    1.46e+05    1.62e+05\n",
       "zip_98116      1.183e+05   5611.307     21.091      0.000    1.07e+05    1.29e+05\n",
       "zip_98117      1.429e+05   4387.114     32.573      0.000    1.34e+05    1.52e+05\n",
       "zip_98118     -1.747e+04   4383.926     -3.985      0.000   -2.61e+04   -8876.384\n",
       "zip_98119      2.659e+05   7878.957     33.748      0.000     2.5e+05    2.81e+05\n",
       "zip_98122      1.418e+05   6020.136     23.562      0.000     1.3e+05    1.54e+05\n",
       "zip_98125      2.153e+04   4998.849      4.308      0.000    1.17e+04    3.13e+04\n",
       "zip_98126      9027.8145   5241.439      1.722      0.085   -1245.883    1.93e+04\n",
       "zip_98133     -2.648e+04   4534.051     -5.840      0.000   -3.54e+04   -1.76e+04\n",
       "zip_98136      6.978e+04   6182.895     11.286      0.000    5.77e+04    8.19e+04\n",
       "zip_98144      7.094e+04   5473.044     12.962      0.000    6.02e+04    8.17e+04\n",
       "zip_98146     -7.699e+04   5708.785    -13.485      0.000   -8.82e+04   -6.58e+04\n",
       "zip_98148     -1.259e+05   1.29e+04     -9.786      0.000   -1.51e+05   -1.01e+05\n",
       "zip_98155     -4.319e+04   4590.220     -9.409      0.000   -5.22e+04   -3.42e+04\n",
       "zip_98166     -9.094e+04   6085.939    -14.943      0.000   -1.03e+05    -7.9e+04\n",
       "zip_98168     -1.355e+05   5911.938    -22.924      0.000   -1.47e+05   -1.24e+05\n",
       "zip_98177       4.16e+04   6147.864      6.766      0.000    2.95e+04    5.36e+04\n",
       "zip_98178     -1.384e+05   5942.182    -23.283      0.000    -1.5e+05   -1.27e+05\n",
       "zip_98188     -1.506e+05   8267.916    -18.219      0.000   -1.67e+05   -1.34e+05\n",
       "zip_98198     -1.718e+05   5775.203    -29.744      0.000   -1.83e+05    -1.6e+05\n",
       "zip_98199      1.947e+05   5665.782     34.366      0.000    1.84e+05    2.06e+05\n",
       "==============================================================================\n",
       "Omnibus:                     3571.303   Durbin-Watson:                   1.981\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            19833.706\n",
       "Skew:                           0.806   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.776   Cond. No.                     6.47e+19\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.8e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_int = sm.add_constant(X)\n",
    "model = sm.OLS(y,X_int).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL PERFORMANCE¶\n",
    "Both the Multiple R-Squared and Adjusted R-Squared values are measures of model performance. Possible values range from 0.0 to 1.0. The Adjusted R-Squared value is always a bit lower than the Multiple R-Squared value, because it reflects model complexity (the number of variables) as it relates to the data and is consequently a more accurate measure of model performance. Adding an additional explanatory variable to the model will likely increase the Multiple R-Squared value but may decrease the Adjusted R-Squared value.\n",
    "\n",
    "An Adjusted R-Squared value of 0.65 would indicate that the current model (the explanatory variables modeled using linear regression) explains approximately 65 percent of the variation in the dependent variable. Said another way, the current model explain approximately 65 percent of the house price.\n",
    "\n",
    "MODEL SIGNIFICANCE\n",
    "The F-test sums the predictive power of all independent variables and determines that it is unlikely that all of the coefficients equal zero. However, it is possible that each variable is not predictive enough on its own to be statistically significant. In other words, the sample provides sufficient evidence to conclude that the current model is significant, but not enough to conclude that any individual variable is significant. This finding is good news because it is sufficiently high.\n",
    "\n",
    "EXPLANATORY POWER OF INDEPENDENT VARIABLES\n",
    "The p-value for all independent variables is less than the significance level, which means that the current independent variables provide sufficient evidence to conclude that the regression model fits the data better than the model with no independent variables. This finding is good news because it means that the independent variables in the current model improve the fit.\n",
    "\n",
    "A read of the coefficient column shows three variables are negatively related with house sale price i.e.\n",
    "\n",
    "Number of bedrooms - this seems counter-intuitive because typically a higher number of bedrooms means more space for family. Perhaps the number of bedrooms increase has a positive benefits to homebuyer up to a certain threshold i.e. why do I want more than 6 bedrooms if I have only 2 children.\n",
    "Year built - it looks like homebuyers in King County value newer houses.\n",
    "Zipcode - we can ignore this because it is no relevant.\n",
    "MODEL BIAS\n",
    "The Jarque-Bera (JB) statistic indicates whether or not the residuals (i.e. the known dependent variable values minus the predicted values) are normally distributed. Put simply, it is a test to confirm normality.\n",
    "\n",
    "When the p-value (probability) for this test is small (smaller than 0.05 for a 95 percent confidence level, for example), the residuals are not normally distributed, indicating the current model is biased. In general, a large JB value, in this current model, indicates that errors are not normally distributed.\n",
    "\n",
    "The Durbin Watson statistic is a number that tests for autocorrelation in the residuals from a statistical regression analysis. The Durbin-Watson statistic is always between 0 and 4. The current model has a value of 2 means that there is no autocorrelation in the sample.\n",
    "\n",
    "MODEL SYMMETRY\n",
    "Skewness implies off-centre (i.e. it means lack of symmetry). The skewness value can be positive or negative, or even undefined. If skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. As a general rule of thumb: If skewness is less than -1 or greater than 1, the distribution is highly skewed. In this current model, it is greater than 1, indicated a highly skewed distribution.\n",
    "\n",
    "Kurtosis refers to the pointedness of a peak in the distribution curve. The value is often compared to the kurtosis of the normal distribution, which is equal to 3. If the kurtosis is greater than 3, then the dataset has heavier tails than a normal distribution (more in the tails). In this current model, the value is greater than 3, indicated a dataset has heavier tails than normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our R-squared is 0.83. Let's try to normalize our continuous values even further with logarithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how close to normal are our continuous data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate in 2 lists: one for continuous columns and one for categorical columns.\n",
    "continuous = ['price',\n",
    " 'sqft_living',\n",
    " 'sqft_lot',\n",
    " 'sqft_above',\n",
    " 'sqft_living15',\n",
    " 'sqft_lot15',\n",
    " 'bathrooms',]\n",
    "categoricals = [ \n",
    " 'view_1',\n",
    " 'view_2',\n",
    " 'view_3',\n",
    " 'view_4',\n",
    " 'condition_3',\n",
    " 'condition_4',\n",
    " 'condition_5',\n",
    " 'floor_1',\n",
    " 'floor_1_5',\n",
    " 'floor_2',\n",
    " 'floor_2_5',\n",
    " 'renovated',\n",
    " 'waterfront',\n",
    " 'gr_5',\n",
    " 'gr_6',\n",
    " 'gr_7',\n",
    " 'gr_8',\n",
    " 'gr_9',\n",
    " 'gr_10',\n",
    " 'basement',\n",
    " 'bedroom_1',\n",
    " 'bedroom_2',\n",
    " 'bedroom_3',\n",
    " 'bedroom_4',\n",
    " 'bedroom_5',\n",
    " 'bedroom_6',\n",
    " 'zip_98001',\n",
    " 'zip_98002',\n",
    " 'zip_98003',\n",
    " 'zip_98004',\n",
    " 'zip_98005',\n",
    " 'zip_98006',\n",
    " 'zip_98007',\n",
    " 'zip_98008',\n",
    " 'zip_98010',\n",
    " 'zip_98011',\n",
    " 'zip_98014',\n",
    " 'zip_98019',\n",
    " 'zip_98022',\n",
    " 'zip_98023',\n",
    " 'zip_98024',\n",
    " 'zip_98027',\n",
    " 'zip_98028',\n",
    " 'zip_98029',\n",
    " 'zip_98030',\n",
    " 'zip_98031',\n",
    " 'zip_98032',\n",
    " 'zip_98033',\n",
    " 'zip_98034',\n",
    " 'zip_98038',\n",
    " 'zip_98039',\n",
    " 'zip_98040',\n",
    " 'zip_98042',\n",
    " 'zip_98045',\n",
    " 'zip_98052',\n",
    " 'zip_98053',\n",
    " 'zip_98055',\n",
    " 'zip_98056',\n",
    " 'zip_98058',\n",
    " 'zip_98059',\n",
    " 'zip_98065',\n",
    " 'zip_98070',\n",
    " 'zip_98072',\n",
    " 'zip_98074',\n",
    " 'zip_98075',\n",
    " 'zip_98077',\n",
    " 'zip_98092',\n",
    " 'zip_98102',\n",
    " 'zip_98103',\n",
    " 'zip_98105',\n",
    " 'zip_98106',\n",
    " 'zip_98107',\n",
    " 'zip_98108',\n",
    " 'zip_98109',\n",
    " 'zip_98112',\n",
    " 'zip_98115',\n",
    " 'zip_98116',\n",
    " 'zip_98117',\n",
    " 'zip_98118',\n",
    " 'zip_98119',\n",
    " 'zip_98122',\n",
    " 'zip_98125',\n",
    " 'zip_98126',\n",
    " 'zip_98133',\n",
    " 'zip_98136',\n",
    " 'zip_98144',\n",
    " 'zip_98146',\n",
    " 'zip_98148',\n",
    " 'zip_98155',\n",
    " 'zip_98166',\n",
    " 'zip_98168',\n",
    " 'zip_98177',\n",
    " 'zip_98178',\n",
    " 'zip_98188',\n",
    " 'zip_98198',\n",
    " 'zip_98199']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_cont = data[continuous]\n",
    "data_cont.hist(figsize=(12,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform and normalize\n",
    "\n",
    "# log features\n",
    "data_names = [f'{column}_log' for column in data_cont.columns]\n",
    "\n",
    "data_log = np.log(data_cont)\n",
    "data_log.columns = data_names\n",
    "\n",
    "# normalize (subract mean and divide by std)\n",
    "\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "data_log_norm = data_log.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check normalization after applying log.\n",
    "data_log_norm.hist(figsize=(12,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All but bathroom column seems more or less normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = data[categoricals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_log</th>\n",
       "      <th>sqft_living_log</th>\n",
       "      <th>sqft_lot_log</th>\n",
       "      <th>sqft_above_log</th>\n",
       "      <th>sqft_living15_log</th>\n",
       "      <th>sqft_lot15_log</th>\n",
       "      <th>bathrooms_log</th>\n",
       "      <th>view_1</th>\n",
       "      <th>view_2</th>\n",
       "      <th>view_3</th>\n",
       "      <th>...</th>\n",
       "      <th>zip_98146</th>\n",
       "      <th>zip_98148</th>\n",
       "      <th>zip_98155</th>\n",
       "      <th>zip_98166</th>\n",
       "      <th>zip_98168</th>\n",
       "      <th>zip_98177</th>\n",
       "      <th>zip_98178</th>\n",
       "      <th>zip_98188</th>\n",
       "      <th>zip_98198</th>\n",
       "      <th>zip_98199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.449182</td>\n",
       "      <td>-1.122302</td>\n",
       "      <td>-0.395968</td>\n",
       "      <td>-0.698166</td>\n",
       "      <td>-1.026332</td>\n",
       "      <td>-0.406035</td>\n",
       "      <td>-1.699163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.467933</td>\n",
       "      <td>0.902761</td>\n",
       "      <td>-0.037679</td>\n",
       "      <td>0.869646</td>\n",
       "      <td>-0.253426</td>\n",
       "      <td>0.082004</td>\n",
       "      <td>0.463854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.902186</td>\n",
       "      <td>-2.232871</td>\n",
       "      <td>0.428058</td>\n",
       "      <td>-1.796742</td>\n",
       "      <td>1.331642</td>\n",
       "      <td>0.169212</td>\n",
       "      <td>-1.699163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.718422</td>\n",
       "      <td>0.197827</td>\n",
       "      <td>-0.572366</td>\n",
       "      <td>-0.998557</td>\n",
       "      <td>-0.976989</td>\n",
       "      <td>-0.603797</td>\n",
       "      <td>1.231197</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.352236</td>\n",
       "      <td>-0.203211</td>\n",
       "      <td>0.120355</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>0.052937</td>\n",
       "      <td>0.149688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   price_log  sqft_living_log  sqft_lot_log  sqft_above_log  \\\n",
       "0  -1.449182        -1.122302     -0.395968       -0.698166   \n",
       "1   0.467933         0.902761     -0.037679        0.869646   \n",
       "2  -1.902186        -2.232871      0.428058       -1.796742   \n",
       "3   0.718422         0.197827     -0.572366       -0.998557   \n",
       "4   0.352236        -0.203211      0.120355        0.211000   \n",
       "\n",
       "   sqft_living15_log  sqft_lot15_log  bathrooms_log  view_1  view_2  view_3  \\\n",
       "0          -1.026332       -0.406035      -1.699163       0       0       0   \n",
       "1          -0.253426        0.082004       0.463854       0       0       0   \n",
       "2           1.331642        0.169212      -1.699163       0       0       0   \n",
       "3          -0.976989       -0.603797       1.231197       0       0       0   \n",
       "4          -0.043401        0.052937       0.149688       0       0       0   \n",
       "\n",
       "   ...  zip_98146  zip_98148  zip_98155  zip_98166  zip_98168  zip_98177  \\\n",
       "0  ...          0          0          0          0          0          0   \n",
       "1  ...          0          0          0          0          0          0   \n",
       "2  ...          0          0          0          0          0          0   \n",
       "3  ...          0          0          0          0          0          0   \n",
       "4  ...          0          0          0          0          0          0   \n",
       "\n",
       "   zip_98178  zip_98188  zip_98198  zip_98199  \n",
       "0          1          0          0          0  \n",
       "1          0          0          0          0  \n",
       "2          0          0          0          0  \n",
       "3          0          0          0          0  \n",
       "4          0          0          0          0  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new data frame with our categorical columns and replace the continuous values with their log.\n",
    "preprocessed = pd.concat([data_log_norm, data_ohe], axis=1)\n",
    "preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our model improves after log normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2nd linear model with price_log as the target variable in statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the price column and set it as y axis and \n",
    "X = preprocessed.drop('price_log', axis=1)\n",
    "y = preprocessed['price_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>price_log</td>    <th>  R-squared:         </th> <td>   0.863</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.862</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1193.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 20 Jun 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:17:25</td>     <th>  Log-Likelihood:    </th> <td> -7993.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 18735</td>      <th>  AIC:               </th> <td>1.619e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 18636</td>      <th>  BIC:               </th> <td>1.696e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    98</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -0.2186</td> <td>    0.021</td> <td>  -10.368</td> <td> 0.000</td> <td>   -0.260</td> <td>   -0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th>   <td>    0.1930</td> <td>    0.011</td> <td>   17.361</td> <td> 0.000</td> <td>    0.171</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>      <td>    0.1003</td> <td>    0.007</td> <td>   15.323</td> <td> 0.000</td> <td>    0.087</td> <td>    0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_above_log</th>    <td>    0.1485</td> <td>    0.011</td> <td>   13.199</td> <td> 0.000</td> <td>    0.126</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15_log</th> <td>    0.1013</td> <td>    0.005</td> <td>   21.859</td> <td> 0.000</td> <td>    0.092</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot15_log</th>    <td>   -0.0313</td> <td>    0.007</td> <td>   -4.750</td> <td> 0.000</td> <td>   -0.044</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms_log</th>     <td>    0.0400</td> <td>    0.005</td> <td>    8.481</td> <td> 0.000</td> <td>    0.031</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_1</th>            <td>    0.2300</td> <td>    0.023</td> <td>    9.893</td> <td> 0.000</td> <td>    0.184</td> <td>    0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_2</th>            <td>    0.2416</td> <td>    0.015</td> <td>   16.470</td> <td> 0.000</td> <td>    0.213</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_3</th>            <td>    0.3798</td> <td>    0.021</td> <td>   17.669</td> <td> 0.000</td> <td>    0.338</td> <td>    0.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_4</th>            <td>    0.6020</td> <td>    0.035</td> <td>   17.392</td> <td> 0.000</td> <td>    0.534</td> <td>    0.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>       <td>    0.2883</td> <td>    0.031</td> <td>    9.157</td> <td> 0.000</td> <td>    0.227</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>       <td>    0.3796</td> <td>    0.032</td> <td>   11.989</td> <td> 0.000</td> <td>    0.318</td> <td>    0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>       <td>    0.5257</td> <td>    0.033</td> <td>   16.055</td> <td> 0.000</td> <td>    0.461</td> <td>    0.590</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_1</th>           <td>   -0.0470</td> <td>    0.011</td> <td>   -4.295</td> <td> 0.000</td> <td>   -0.068</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_1_5</th>         <td>   -0.0280</td> <td>    0.012</td> <td>   -2.295</td> <td> 0.022</td> <td>   -0.052</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_2</th>           <td>   -0.0623</td> <td>    0.011</td> <td>   -5.919</td> <td> 0.000</td> <td>   -0.083</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floor_2_5</th>         <td>   -0.0814</td> <td>    0.029</td> <td>   -2.781</td> <td> 0.005</td> <td>   -0.139</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>renovated</th>         <td>    0.1614</td> <td>    0.016</td> <td>   10.289</td> <td> 0.000</td> <td>    0.131</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>        <td>    0.8341</td> <td>    0.059</td> <td>   14.078</td> <td> 0.000</td> <td>    0.718</td> <td>    0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_5</th>              <td>   -0.4681</td> <td>    0.024</td> <td>  -19.664</td> <td> 0.000</td> <td>   -0.515</td> <td>   -0.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_6</th>              <td>   -0.3024</td> <td>    0.011</td> <td>  -27.320</td> <td> 0.000</td> <td>   -0.324</td> <td>   -0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_7</th>              <td>   -0.1527</td> <td>    0.008</td> <td>  -19.989</td> <td> 0.000</td> <td>   -0.168</td> <td>   -0.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_8</th>              <td>    0.0302</td> <td>    0.008</td> <td>    3.711</td> <td> 0.000</td> <td>    0.014</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_9</th>              <td>    0.2669</td> <td>    0.011</td> <td>   24.424</td> <td> 0.000</td> <td>    0.246</td> <td>    0.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_10</th>             <td>    0.4074</td> <td>    0.016</td> <td>   25.427</td> <td> 0.000</td> <td>    0.376</td> <td>    0.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>basement</th>          <td>    0.0600</td> <td>    0.012</td> <td>    4.923</td> <td> 0.000</td> <td>    0.036</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_1</th>         <td>    0.0230</td> <td>    0.027</td> <td>    0.854</td> <td> 0.393</td> <td>   -0.030</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_2</th>         <td>    0.0004</td> <td>    0.011</td> <td>    0.035</td> <td> 0.972</td> <td>   -0.021</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_3</th>         <td>   -0.0361</td> <td>    0.008</td> <td>   -4.349</td> <td> 0.000</td> <td>   -0.052</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_4</th>         <td>   -0.0324</td> <td>    0.009</td> <td>   -3.633</td> <td> 0.000</td> <td>   -0.050</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_5</th>         <td>   -0.0728</td> <td>    0.012</td> <td>   -5.900</td> <td> 0.000</td> <td>   -0.097</td> <td>   -0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_6</th>         <td>   -0.1006</td> <td>    0.025</td> <td>   -4.105</td> <td> 0.000</td> <td>   -0.149</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98001</th>         <td>   -1.0552</td> <td>    0.020</td> <td>  -52.226</td> <td> 0.000</td> <td>   -1.095</td> <td>   -1.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98002</th>         <td>   -1.0575</td> <td>    0.027</td> <td>  -39.444</td> <td> 0.000</td> <td>   -1.110</td> <td>   -1.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98003</th>         <td>   -0.9986</td> <td>    0.023</td> <td>  -43.702</td> <td> 0.000</td> <td>   -1.043</td> <td>   -0.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98004</th>         <td>    1.3035</td> <td>    0.027</td> <td>   48.512</td> <td> 0.000</td> <td>    1.251</td> <td>    1.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98005</th>         <td>    0.5765</td> <td>    0.031</td> <td>   18.764</td> <td> 0.000</td> <td>    0.516</td> <td>    0.637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98006</th>         <td>    0.3620</td> <td>    0.019</td> <td>   18.798</td> <td> 0.000</td> <td>    0.324</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98007</th>         <td>    0.3802</td> <td>    0.032</td> <td>   11.785</td> <td> 0.000</td> <td>    0.317</td> <td>    0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98008</th>         <td>    0.3694</td> <td>    0.023</td> <td>   16.026</td> <td> 0.000</td> <td>    0.324</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98010</th>         <td>   -0.5490</td> <td>    0.043</td> <td>  -12.832</td> <td> 0.000</td> <td>   -0.633</td> <td>   -0.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98011</th>         <td>   -0.0814</td> <td>    0.027</td> <td>   -2.981</td> <td> 0.003</td> <td>   -0.135</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98014</th>         <td>   -0.3536</td> <td>    0.043</td> <td>   -8.189</td> <td> 0.000</td> <td>   -0.438</td> <td>   -0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98019</th>         <td>   -0.3636</td> <td>    0.030</td> <td>  -12.221</td> <td> 0.000</td> <td>   -0.422</td> <td>   -0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98022</th>         <td>   -1.0079</td> <td>    0.028</td> <td>  -35.730</td> <td> 0.000</td> <td>   -1.063</td> <td>   -0.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98023</th>         <td>   -1.1006</td> <td>    0.017</td> <td>  -63.567</td> <td> 0.000</td> <td>   -1.135</td> <td>   -1.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98024</th>         <td>   -0.1437</td> <td>    0.059</td> <td>   -2.431</td> <td> 0.015</td> <td>   -0.260</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98027</th>         <td>    0.1416</td> <td>    0.022</td> <td>    6.454</td> <td> 0.000</td> <td>    0.099</td> <td>    0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98028</th>         <td>   -0.1443</td> <td>    0.023</td> <td>   -6.385</td> <td> 0.000</td> <td>   -0.189</td> <td>   -0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98029</th>         <td>    0.2642</td> <td>    0.022</td> <td>   11.853</td> <td> 0.000</td> <td>    0.221</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98030</th>         <td>   -0.9489</td> <td>    0.024</td> <td>  -39.882</td> <td> 0.000</td> <td>   -0.996</td> <td>   -0.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98031</th>         <td>   -0.8890</td> <td>    0.023</td> <td>  -38.462</td> <td> 0.000</td> <td>   -0.934</td> <td>   -0.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98032</th>         <td>   -1.0784</td> <td>    0.034</td> <td>  -32.096</td> <td> 0.000</td> <td>   -1.144</td> <td>   -1.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98033</th>         <td>    0.6283</td> <td>    0.019</td> <td>   32.580</td> <td> 0.000</td> <td>    0.590</td> <td>    0.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98034</th>         <td>    0.1149</td> <td>    0.017</td> <td>    6.848</td> <td> 0.000</td> <td>    0.082</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98038</th>         <td>   -0.7097</td> <td>    0.017</td> <td>  -41.705</td> <td> 0.000</td> <td>   -0.743</td> <td>   -0.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98039</th>         <td>    1.6727</td> <td>    0.098</td> <td>   17.037</td> <td> 0.000</td> <td>    1.480</td> <td>    1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98040</th>         <td>    0.8628</td> <td>    0.027</td> <td>   31.751</td> <td> 0.000</td> <td>    0.810</td> <td>    0.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98042</th>         <td>   -0.9252</td> <td>    0.017</td> <td>  -55.078</td> <td> 0.000</td> <td>   -0.958</td> <td>   -0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98045</th>         <td>   -0.3329</td> <td>    0.028</td> <td>  -11.881</td> <td> 0.000</td> <td>   -0.388</td> <td>   -0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98052</th>         <td>    0.3470</td> <td>    0.017</td> <td>   20.892</td> <td> 0.000</td> <td>    0.314</td> <td>    0.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98053</th>         <td>    0.2759</td> <td>    0.022</td> <td>   12.619</td> <td> 0.000</td> <td>    0.233</td> <td>    0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98055</th>         <td>   -0.7325</td> <td>    0.023</td> <td>  -31.488</td> <td> 0.000</td> <td>   -0.778</td> <td>   -0.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98056</th>         <td>   -0.3481</td> <td>    0.019</td> <td>  -18.157</td> <td> 0.000</td> <td>   -0.386</td> <td>   -0.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98058</th>         <td>   -0.6978</td> <td>    0.018</td> <td>  -38.249</td> <td> 0.000</td> <td>   -0.734</td> <td>   -0.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98059</th>         <td>   -0.3506</td> <td>    0.019</td> <td>  -18.641</td> <td> 0.000</td> <td>   -0.387</td> <td>   -0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98065</th>         <td>   -0.1333</td> <td>    0.024</td> <td>   -5.669</td> <td> 0.000</td> <td>   -0.179</td> <td>   -0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98070</th>         <td>   -0.4111</td> <td>    0.048</td> <td>   -8.585</td> <td> 0.000</td> <td>   -0.505</td> <td>   -0.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98072</th>         <td>   -0.0176</td> <td>    0.025</td> <td>   -0.714</td> <td> 0.475</td> <td>   -0.066</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98074</th>         <td>    0.1689</td> <td>    0.020</td> <td>    8.532</td> <td> 0.000</td> <td>    0.130</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98075</th>         <td>    0.2015</td> <td>    0.023</td> <td>    8.635</td> <td> 0.000</td> <td>    0.156</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98077</th>         <td>   -0.0922</td> <td>    0.033</td> <td>   -2.778</td> <td> 0.005</td> <td>   -0.157</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98092</th>         <td>   -1.0438</td> <td>    0.022</td> <td>  -47.525</td> <td> 0.000</td> <td>   -1.087</td> <td>   -1.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98102</th>         <td>    1.0905</td> <td>    0.042</td> <td>   25.759</td> <td> 0.000</td> <td>    1.008</td> <td>    1.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98103</th>         <td>    0.9103</td> <td>    0.019</td> <td>   48.058</td> <td> 0.000</td> <td>    0.873</td> <td>    0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98105</th>         <td>    1.0362</td> <td>    0.027</td> <td>   37.838</td> <td> 0.000</td> <td>    0.982</td> <td>    1.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98106</th>         <td>   -0.2184</td> <td>    0.021</td> <td>  -10.158</td> <td> 0.000</td> <td>   -0.260</td> <td>   -0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98107</th>         <td>    0.8765</td> <td>    0.027</td> <td>   32.285</td> <td> 0.000</td> <td>    0.823</td> <td>    0.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98108</th>         <td>   -0.2031</td> <td>    0.028</td> <td>   -7.309</td> <td> 0.000</td> <td>   -0.258</td> <td>   -0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98109</th>         <td>    1.1332</td> <td>    0.040</td> <td>   28.377</td> <td> 0.000</td> <td>    1.055</td> <td>    1.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98112</th>         <td>    1.1683</td> <td>    0.027</td> <td>   42.537</td> <td> 0.000</td> <td>    1.114</td> <td>    1.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98115</th>         <td>    0.7803</td> <td>    0.017</td> <td>   46.276</td> <td> 0.000</td> <td>    0.747</td> <td>    0.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98116</th>         <td>    0.6833</td> <td>    0.022</td> <td>   30.517</td> <td> 0.000</td> <td>    0.639</td> <td>    0.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98117</th>         <td>    0.8153</td> <td>    0.018</td> <td>   46.427</td> <td> 0.000</td> <td>    0.781</td> <td>    0.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98118</th>         <td>    0.0187</td> <td>    0.017</td> <td>    1.071</td> <td> 0.284</td> <td>   -0.016</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98119</th>         <td>    1.1237</td> <td>    0.031</td> <td>   35.690</td> <td> 0.000</td> <td>    1.062</td> <td>    1.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98122</th>         <td>    0.7519</td> <td>    0.024</td> <td>   31.090</td> <td> 0.000</td> <td>    0.705</td> <td>    0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98125</th>         <td>    0.2395</td> <td>    0.020</td> <td>   12.073</td> <td> 0.000</td> <td>    0.201</td> <td>    0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98126</th>         <td>    0.2249</td> <td>    0.021</td> <td>   10.743</td> <td> 0.000</td> <td>    0.184</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98133</th>         <td>   -0.0013</td> <td>    0.018</td> <td>   -0.074</td> <td> 0.941</td> <td>   -0.037</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98136</th>         <td>    0.4852</td> <td>    0.025</td> <td>   19.711</td> <td> 0.000</td> <td>    0.437</td> <td>    0.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98144</th>         <td>    0.4522</td> <td>    0.022</td> <td>   20.510</td> <td> 0.000</td> <td>    0.409</td> <td>    0.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98146</th>         <td>   -0.4034</td> <td>    0.023</td> <td>  -17.791</td> <td> 0.000</td> <td>   -0.448</td> <td>   -0.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98148</th>         <td>   -0.7018</td> <td>    0.051</td> <td>  -13.737</td> <td> 0.000</td> <td>   -0.802</td> <td>   -0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98155</th>         <td>   -0.0942</td> <td>    0.018</td> <td>   -5.158</td> <td> 0.000</td> <td>   -0.130</td> <td>   -0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98166</th>         <td>   -0.3463</td> <td>    0.024</td> <td>  -14.307</td> <td> 0.000</td> <td>   -0.394</td> <td>   -0.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98168</th>         <td>   -0.8427</td> <td>    0.024</td> <td>  -35.802</td> <td> 0.000</td> <td>   -0.889</td> <td>   -0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98177</th>         <td>    0.2463</td> <td>    0.024</td> <td>   10.088</td> <td> 0.000</td> <td>    0.198</td> <td>    0.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98178</th>         <td>   -0.6892</td> <td>    0.024</td> <td>  -29.204</td> <td> 0.000</td> <td>   -0.735</td> <td>   -0.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98188</th>         <td>   -0.8129</td> <td>    0.033</td> <td>  -24.742</td> <td> 0.000</td> <td>   -0.877</td> <td>   -0.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98198</th>         <td>   -0.8950</td> <td>    0.023</td> <td>  -39.024</td> <td> 0.000</td> <td>   -0.940</td> <td>   -0.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98199</th>         <td>    0.8504</td> <td>    0.023</td> <td>   37.625</td> <td> 0.000</td> <td>    0.806</td> <td>    0.895</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1512.112</td> <th>  Durbin-Watson:     </th> <td>   1.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6133.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.320</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.729</td>  <th>  Cond. No.          </th> <td>1.23e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 4.42e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              price_log   R-squared:                       0.863\n",
       "Model:                            OLS   Adj. R-squared:                  0.862\n",
       "Method:                 Least Squares   F-statistic:                     1193.\n",
       "Date:                Sat, 20 Jun 2020   Prob (F-statistic):               0.00\n",
       "Time:                        19:17:25   Log-Likelihood:                -7993.9\n",
       "No. Observations:               18735   AIC:                         1.619e+04\n",
       "Df Residuals:                   18636   BIC:                         1.696e+04\n",
       "Df Model:                          98                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                -0.2186      0.021    -10.368      0.000      -0.260      -0.177\n",
       "sqft_living_log       0.1930      0.011     17.361      0.000       0.171       0.215\n",
       "sqft_lot_log          0.1003      0.007     15.323      0.000       0.087       0.113\n",
       "sqft_above_log        0.1485      0.011     13.199      0.000       0.126       0.171\n",
       "sqft_living15_log     0.1013      0.005     21.859      0.000       0.092       0.110\n",
       "sqft_lot15_log       -0.0313      0.007     -4.750      0.000      -0.044      -0.018\n",
       "bathrooms_log         0.0400      0.005      8.481      0.000       0.031       0.049\n",
       "view_1                0.2300      0.023      9.893      0.000       0.184       0.276\n",
       "view_2                0.2416      0.015     16.470      0.000       0.213       0.270\n",
       "view_3                0.3798      0.021     17.669      0.000       0.338       0.422\n",
       "view_4                0.6020      0.035     17.392      0.000       0.534       0.670\n",
       "condition_3           0.2883      0.031      9.157      0.000       0.227       0.350\n",
       "condition_4           0.3796      0.032     11.989      0.000       0.318       0.442\n",
       "condition_5           0.5257      0.033     16.055      0.000       0.461       0.590\n",
       "floor_1              -0.0470      0.011     -4.295      0.000      -0.068      -0.026\n",
       "floor_1_5            -0.0280      0.012     -2.295      0.022      -0.052      -0.004\n",
       "floor_2              -0.0623      0.011     -5.919      0.000      -0.083      -0.042\n",
       "floor_2_5            -0.0814      0.029     -2.781      0.005      -0.139      -0.024\n",
       "renovated             0.1614      0.016     10.289      0.000       0.131       0.192\n",
       "waterfront            0.8341      0.059     14.078      0.000       0.718       0.950\n",
       "gr_5                 -0.4681      0.024    -19.664      0.000      -0.515      -0.421\n",
       "gr_6                 -0.3024      0.011    -27.320      0.000      -0.324      -0.281\n",
       "gr_7                 -0.1527      0.008    -19.989      0.000      -0.168      -0.138\n",
       "gr_8                  0.0302      0.008      3.711      0.000       0.014       0.046\n",
       "gr_9                  0.2669      0.011     24.424      0.000       0.246       0.288\n",
       "gr_10                 0.4074      0.016     25.427      0.000       0.376       0.439\n",
       "basement              0.0600      0.012      4.923      0.000       0.036       0.084\n",
       "bedroom_1             0.0230      0.027      0.854      0.393      -0.030       0.076\n",
       "bedroom_2             0.0004      0.011      0.035      0.972      -0.021       0.021\n",
       "bedroom_3            -0.0361      0.008     -4.349      0.000      -0.052      -0.020\n",
       "bedroom_4            -0.0324      0.009     -3.633      0.000      -0.050      -0.015\n",
       "bedroom_5            -0.0728      0.012     -5.900      0.000      -0.097      -0.049\n",
       "bedroom_6            -0.1006      0.025     -4.105      0.000      -0.149      -0.053\n",
       "zip_98001            -1.0552      0.020    -52.226      0.000      -1.095      -1.016\n",
       "zip_98002            -1.0575      0.027    -39.444      0.000      -1.110      -1.005\n",
       "zip_98003            -0.9986      0.023    -43.702      0.000      -1.043      -0.954\n",
       "zip_98004             1.3035      0.027     48.512      0.000       1.251       1.356\n",
       "zip_98005             0.5765      0.031     18.764      0.000       0.516       0.637\n",
       "zip_98006             0.3620      0.019     18.798      0.000       0.324       0.400\n",
       "zip_98007             0.3802      0.032     11.785      0.000       0.317       0.443\n",
       "zip_98008             0.3694      0.023     16.026      0.000       0.324       0.415\n",
       "zip_98010            -0.5490      0.043    -12.832      0.000      -0.633      -0.465\n",
       "zip_98011            -0.0814      0.027     -2.981      0.003      -0.135      -0.028\n",
       "zip_98014            -0.3536      0.043     -8.189      0.000      -0.438      -0.269\n",
       "zip_98019            -0.3636      0.030    -12.221      0.000      -0.422      -0.305\n",
       "zip_98022            -1.0079      0.028    -35.730      0.000      -1.063      -0.953\n",
       "zip_98023            -1.1006      0.017    -63.567      0.000      -1.135      -1.067\n",
       "zip_98024            -0.1437      0.059     -2.431      0.015      -0.260      -0.028\n",
       "zip_98027             0.1416      0.022      6.454      0.000       0.099       0.185\n",
       "zip_98028            -0.1443      0.023     -6.385      0.000      -0.189      -0.100\n",
       "zip_98029             0.2642      0.022     11.853      0.000       0.221       0.308\n",
       "zip_98030            -0.9489      0.024    -39.882      0.000      -0.996      -0.902\n",
       "zip_98031            -0.8890      0.023    -38.462      0.000      -0.934      -0.844\n",
       "zip_98032            -1.0784      0.034    -32.096      0.000      -1.144      -1.013\n",
       "zip_98033             0.6283      0.019     32.580      0.000       0.590       0.666\n",
       "zip_98034             0.1149      0.017      6.848      0.000       0.082       0.148\n",
       "zip_98038            -0.7097      0.017    -41.705      0.000      -0.743      -0.676\n",
       "zip_98039             1.6727      0.098     17.037      0.000       1.480       1.865\n",
       "zip_98040             0.8628      0.027     31.751      0.000       0.810       0.916\n",
       "zip_98042            -0.9252      0.017    -55.078      0.000      -0.958      -0.892\n",
       "zip_98045            -0.3329      0.028    -11.881      0.000      -0.388      -0.278\n",
       "zip_98052             0.3470      0.017     20.892      0.000       0.314       0.380\n",
       "zip_98053             0.2759      0.022     12.619      0.000       0.233       0.319\n",
       "zip_98055            -0.7325      0.023    -31.488      0.000      -0.778      -0.687\n",
       "zip_98056            -0.3481      0.019    -18.157      0.000      -0.386      -0.310\n",
       "zip_98058            -0.6978      0.018    -38.249      0.000      -0.734      -0.662\n",
       "zip_98059            -0.3506      0.019    -18.641      0.000      -0.387      -0.314\n",
       "zip_98065            -0.1333      0.024     -5.669      0.000      -0.179      -0.087\n",
       "zip_98070            -0.4111      0.048     -8.585      0.000      -0.505      -0.317\n",
       "zip_98072            -0.0176      0.025     -0.714      0.475      -0.066       0.031\n",
       "zip_98074             0.1689      0.020      8.532      0.000       0.130       0.208\n",
       "zip_98075             0.2015      0.023      8.635      0.000       0.156       0.247\n",
       "zip_98077            -0.0922      0.033     -2.778      0.005      -0.157      -0.027\n",
       "zip_98092            -1.0438      0.022    -47.525      0.000      -1.087      -1.001\n",
       "zip_98102             1.0905      0.042     25.759      0.000       1.008       1.174\n",
       "zip_98103             0.9103      0.019     48.058      0.000       0.873       0.947\n",
       "zip_98105             1.0362      0.027     37.838      0.000       0.982       1.090\n",
       "zip_98106            -0.2184      0.021    -10.158      0.000      -0.260      -0.176\n",
       "zip_98107             0.8765      0.027     32.285      0.000       0.823       0.930\n",
       "zip_98108            -0.2031      0.028     -7.309      0.000      -0.258      -0.149\n",
       "zip_98109             1.1332      0.040     28.377      0.000       1.055       1.211\n",
       "zip_98112             1.1683      0.027     42.537      0.000       1.114       1.222\n",
       "zip_98115             0.7803      0.017     46.276      0.000       0.747       0.813\n",
       "zip_98116             0.6833      0.022     30.517      0.000       0.639       0.727\n",
       "zip_98117             0.8153      0.018     46.427      0.000       0.781       0.850\n",
       "zip_98118             0.0187      0.017      1.071      0.284      -0.016       0.053\n",
       "zip_98119             1.1237      0.031     35.690      0.000       1.062       1.185\n",
       "zip_98122             0.7519      0.024     31.090      0.000       0.705       0.799\n",
       "zip_98125             0.2395      0.020     12.073      0.000       0.201       0.278\n",
       "zip_98126             0.2249      0.021     10.743      0.000       0.184       0.266\n",
       "zip_98133            -0.0013      0.018     -0.074      0.941      -0.037       0.034\n",
       "zip_98136             0.4852      0.025     19.711      0.000       0.437       0.533\n",
       "zip_98144             0.4522      0.022     20.510      0.000       0.409       0.495\n",
       "zip_98146            -0.4034      0.023    -17.791      0.000      -0.448      -0.359\n",
       "zip_98148            -0.7018      0.051    -13.737      0.000      -0.802      -0.602\n",
       "zip_98155            -0.0942      0.018     -5.158      0.000      -0.130      -0.058\n",
       "zip_98166            -0.3463      0.024    -14.307      0.000      -0.394      -0.299\n",
       "zip_98168            -0.8427      0.024    -35.802      0.000      -0.889      -0.797\n",
       "zip_98177             0.2463      0.024     10.088      0.000       0.198       0.294\n",
       "zip_98178            -0.6892      0.024    -29.204      0.000      -0.735      -0.643\n",
       "zip_98188            -0.8129      0.033    -24.742      0.000      -0.877      -0.748\n",
       "zip_98198            -0.8950      0.023    -39.024      0.000      -0.940      -0.850\n",
       "zip_98199             0.8504      0.023     37.625      0.000       0.806       0.895\n",
       "==============================================================================\n",
       "Omnibus:                     1512.112   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6133.878\n",
       "Skew:                          -0.320   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.729   Cond. No.                     1.23e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 4.42e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_int = sm.add_constant(X)\n",
    "model = sm.OLS(y,X_int).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.93000639e-01,  1.00302704e-01,  1.48513526e-01,  1.01308042e-01,\n",
       "       -3.13572518e-02,  4.00369346e-02,  2.30051744e-01,  2.41593153e-01,\n",
       "        3.79871206e-01,  6.02069979e-01,  2.88182609e-01,  3.79551262e-01,\n",
       "        5.25655088e-01, -6.63764370e+10, -6.63764370e+10, -6.63764370e+10,\n",
       "       -6.63764370e+10,  1.61377799e-01,  8.34272647e-01,  6.18102744e+09,\n",
       "        6.18102744e+09,  6.18102744e+09,  6.18102744e+09,  6.18102744e+09,\n",
       "        6.18102744e+09,  6.00331341e-02,  1.47318398e+10,  1.47318398e+10,\n",
       "        1.47318398e+10,  1.47318398e+10,  1.47318398e+10,  1.47318398e+10,\n",
       "        7.86023659e+08,  7.86023659e+08,  7.86023659e+08,  7.86023661e+08,\n",
       "        7.86023660e+08,  7.86023660e+08,  7.86023660e+08,  7.86023660e+08,\n",
       "        7.86023659e+08,  7.86023660e+08,  7.86023660e+08,  7.86023659e+08,\n",
       "        7.86023659e+08,  7.86023659e+08,  7.86023660e+08,  7.86023660e+08,\n",
       "        7.86023660e+08,  7.86023660e+08,  7.86023659e+08,  7.86023659e+08,\n",
       "        7.86023659e+08,  7.86023660e+08,  7.86023660e+08,  7.86023659e+08,\n",
       "        7.86023662e+08,  7.86023661e+08,  7.86023659e+08,  7.86023660e+08,\n",
       "        7.86023660e+08,  7.86023660e+08,  7.86023659e+08,  7.86023660e+08,\n",
       "        7.86023659e+08,  7.86023660e+08,  7.86023660e+08,  7.86023659e+08,\n",
       "        7.86023660e+08,  7.86023660e+08,  7.86023660e+08,  7.86023660e+08,\n",
       "        7.86023659e+08,  7.86023661e+08,  7.86023661e+08,  7.86023661e+08,\n",
       "        7.86023660e+08,  7.86023661e+08,  7.86023660e+08,  7.86023661e+08,\n",
       "        7.86023661e+08,  7.86023661e+08,  7.86023661e+08,  7.86023661e+08,\n",
       "        7.86023660e+08,  7.86023661e+08,  7.86023661e+08,  7.86023660e+08,\n",
       "        7.86023660e+08,  7.86023660e+08,  7.86023660e+08,  7.86023660e+08,\n",
       "        7.86023659e+08,  7.86023659e+08,  7.86023660e+08,  7.86023660e+08,\n",
       "        7.86023659e+08,  7.86023660e+08,  7.86023659e+08,  7.86023659e+08,\n",
       "        7.86023659e+08,  7.86023661e+08])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coefficients\n",
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 3rd linear model with Price_log as the target variable in statsmodels after we chose the best feature based on p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed.drop('price_log', axis=1)\n",
    "y = preprocessed['price_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'price_log'\n",
    "predictors = preprocessed.drop('price_log', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  floor_1                        with p-value 0.0\n",
      "Add  basement                       with p-value 0.0\n",
      "Add  sqft_above_log                 with p-value 0.0\n",
      "Add  sqft_living15_log              with p-value 1.7636e-261\n",
      "Add  zip_98023                      with p-value 2.16349e-206\n",
      "Add  zip_98042                      with p-value 1.07879e-155\n",
      "Add  zip_98001                      with p-value 1.79123e-155\n",
      "Add  zip_98038                      with p-value 2.99196e-141\n",
      "Add  zip_98092                      with p-value 4.97249e-154\n",
      "Add  zip_98003                      with p-value 3.00182e-131\n",
      "Add  zip_98030                      with p-value 6.63731e-136\n",
      "Add  zip_98058                      with p-value 1.85029e-137\n",
      "Add  zip_98031                      with p-value 5.58559e-145\n",
      "Add  zip_98168                      with p-value 2.00285e-144\n",
      "Add  zip_98002                      with p-value 7.4377e-148\n",
      "Add  zip_98198                      with p-value 3.341e-147\n",
      "Add  zip_98055                      with p-value 1.3163e-145\n",
      "Add  zip_98004                      with p-value 8.31495e-136\n",
      "Add  zip_98022                      with p-value 3.04899e-128\n",
      "Add  zip_98178                      with p-value 4.14852e-132\n",
      "Add  zip_98032                      with p-value 4.61857e-137\n",
      "Add  zip_98059                      with p-value 7.98877e-113\n",
      "Add  zip_98103                      with p-value 2.01426e-108\n",
      "Add  zip_98117                      with p-value 2.3623e-107\n",
      "Add  zip_98115                      with p-value 8.40735e-105\n",
      "Add  zip_98112                      with p-value 1.23922e-111\n",
      "Add  zip_98188                      with p-value 9.00962e-103\n",
      "Add  zip_98199                      with p-value 1.13184e-97\n",
      "Add  zip_98105                      with p-value 4.74403e-97\n",
      "Drop floor_1                        with p-value 0.459422\n",
      "Add  zip_98119                      with p-value 1.4581e-101\n",
      "Add  zip_98040                      with p-value 2.98468e-105\n",
      "Add  view_4                         with p-value 2.361e-108\n",
      "Add  zip_98033                      with p-value 1.6585e-93\n",
      "Add  zip_98107                      with p-value 8.1166e-94\n",
      "Add  zip_98116                      with p-value 1.56842e-97\n",
      "Add  zip_98122                      with p-value 1.72459e-98\n",
      "Add  zip_98109                      with p-value 7.2117e-91\n",
      "Add  sqft_living_log                with p-value 9.7593e-96\n",
      "Add  gr_9                           with p-value 1.82798e-84\n",
      "Add  gr_10                          with p-value 4.36928e-103\n",
      "Add  gr_8                           with p-value 4.71549e-146\n",
      "Add  condition_3                    with p-value 1.51792e-103\n",
      "Add  zip_98056                      with p-value 1.10372e-84\n",
      "Add  gr_7                           with p-value 8.55005e-78\n",
      "Add  zip_98102                      with p-value 2.03528e-72\n",
      "Add  zip_98146                      with p-value 4.91124e-63\n",
      "Add  zip_98136                      with p-value 1.61152e-51\n",
      "Add  view_3                         with p-value 1.70452e-46\n",
      "Add  view_2                         with p-value 4.35327e-48\n",
      "Add  zip_98166                      with p-value 8.12588e-48\n",
      "Add  zip_98039                      with p-value 3.76029e-46\n",
      "Add  zip_98005                      with p-value 4.30094e-44\n",
      "Add  zip_98106                      with p-value 9.15551e-43\n",
      "Add  zip_98148                      with p-value 3.88409e-43\n",
      "Add  zip_98019                      with p-value 9.52015e-44\n",
      "Add  zip_98045                      with p-value 2.29679e-39\n",
      "Add  sqft_lot_log                   with p-value 1.41957e-38\n",
      "Add  zip_98144                      with p-value 5.03256e-49\n",
      "Add  zip_98010                      with p-value 2.88587e-43\n",
      "Add  condition_5                    with p-value 9.38676e-39\n",
      "Add  zip_98052                      with p-value 3.9704e-39\n",
      "Add  zip_98006                      with p-value 5.33082e-36\n",
      "Add  waterfront                     with p-value 3.80402e-35\n",
      "Add  zip_98008                      with p-value 1.06853e-34\n",
      "Add  condition_4                    with p-value 1.92982e-32\n",
      "Add  renovated                      with p-value 4.17685e-26\n",
      "Add  zip_98053                      with p-value 6.76399e-24\n",
      "Add  view_1                         with p-value 1.32582e-21\n",
      "Add  zip_98007                      with p-value 5.82364e-21\n",
      "Add  zip_98029                      with p-value 1.19165e-22\n",
      "Add  zip_98125                      with p-value 5.5467e-24\n",
      "Add  zip_98126                      with p-value 3.01715e-25\n",
      "Add  zip_98177                      with p-value 5.97864e-21\n",
      "Add  zip_98075                      with p-value 2.11566e-18\n",
      "Add  zip_98074                      with p-value 1.15746e-22\n",
      "Add  zip_98034                      with p-value 2.98702e-23\n",
      "Add  zip_98027                      with p-value 1.6845e-22\n",
      "Add  bathrooms_log                  with p-value 1.72835e-11\n",
      "Add  zip_98070                      with p-value 5.92068e-12\n",
      "Add  zip_98014                      with p-value 9.05059e-12\n",
      "Add  gr_6                           with p-value 2.81145e-10\n",
      "Add  gr_5                           with p-value 9.45363e-94\n",
      "Drop gr_8                           with p-value 0.293292\n",
      "Add  zip_98118                      with p-value 5.81092e-09\n",
      "Add  zip_98133                      with p-value 1.27978e-08\n",
      "Add  sqft_lot15_log                 with p-value 9.22312e-06\n",
      "Add  zip_98072                      with p-value 7.87399e-05\n",
      "Add  bedroom_2                      with p-value 0.00274879\n",
      "Add  bedroom_5                      with p-value 0.00245738\n",
      "Add  zip_98108                      with p-value 0.00312768\n",
      "resulting features:\n",
      "['basement', 'sqft_above_log', 'sqft_living15_log', 'zip_98023', 'zip_98042', 'zip_98001', 'zip_98038', 'zip_98092', 'zip_98003', 'zip_98030', 'zip_98058', 'zip_98031', 'zip_98168', 'zip_98002', 'zip_98198', 'zip_98055', 'zip_98004', 'zip_98022', 'zip_98178', 'zip_98032', 'zip_98059', 'zip_98103', 'zip_98117', 'zip_98115', 'zip_98112', 'zip_98188', 'zip_98199', 'zip_98105', 'zip_98119', 'zip_98040', 'view_4', 'zip_98033', 'zip_98107', 'zip_98116', 'zip_98122', 'zip_98109', 'sqft_living_log', 'gr_9', 'gr_10', 'condition_3', 'zip_98056', 'gr_7', 'zip_98102', 'zip_98146', 'zip_98136', 'view_3', 'view_2', 'zip_98166', 'zip_98039', 'zip_98005', 'zip_98106', 'zip_98148', 'zip_98019', 'zip_98045', 'sqft_lot_log', 'zip_98144', 'zip_98010', 'condition_5', 'zip_98052', 'zip_98006', 'waterfront', 'zip_98008', 'condition_4', 'renovated', 'zip_98053', 'view_1', 'zip_98007', 'zip_98029', 'zip_98125', 'zip_98126', 'zip_98177', 'zip_98075', 'zip_98074', 'zip_98034', 'zip_98027', 'bathrooms_log', 'zip_98070', 'zip_98014', 'gr_6', 'gr_5', 'zip_98118', 'zip_98133', 'sqft_lot15_log', 'zip_98072', 'bedroom_2', 'bedroom_5', 'zip_98108']\n"
     ]
    }
   ],
   "source": [
    "result = stepwise_selection(predictors, preprocessed['price_log'], verbose=True)\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>price_log</td>    <th>  R-squared:         </th> <td>   0.862</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.862</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1343.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 20 Jun 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:20:58</td>     <th>  Log-Likelihood:    </th> <td> -8005.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 18735</td>      <th>  AIC:               </th> <td>1.619e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 18647</td>      <th>  BIC:               </th> <td>1.688e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    87</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>   -0.3888</td> <td>    0.034</td> <td>  -11.422</td> <td> 0.000</td> <td>   -0.455</td> <td>   -0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>basement</th>          <td>    0.0613</td> <td>    0.012</td> <td>    5.045</td> <td> 0.000</td> <td>    0.037</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_above_log</th>    <td>    0.1453</td> <td>    0.011</td> <td>   13.770</td> <td> 0.000</td> <td>    0.125</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15_log</th> <td>    0.1017</td> <td>    0.005</td> <td>   22.147</td> <td> 0.000</td> <td>    0.093</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98023</th>         <td>   -0.9883</td> <td>    0.020</td> <td>  -49.324</td> <td> 0.000</td> <td>   -1.028</td> <td>   -0.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98042</th>         <td>   -0.8150</td> <td>    0.019</td> <td>  -41.807</td> <td> 0.000</td> <td>   -0.853</td> <td>   -0.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98001</th>         <td>   -0.9439</td> <td>    0.023</td> <td>  -41.833</td> <td> 0.000</td> <td>   -0.988</td> <td>   -0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98038</th>         <td>   -0.5985</td> <td>    0.020</td> <td>  -30.539</td> <td> 0.000</td> <td>   -0.637</td> <td>   -0.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98092</th>         <td>   -0.9328</td> <td>    0.024</td> <td>  -38.584</td> <td> 0.000</td> <td>   -0.980</td> <td>   -0.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98003</th>         <td>   -0.8862</td> <td>    0.025</td> <td>  -35.310</td> <td> 0.000</td> <td>   -0.935</td> <td>   -0.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98030</th>         <td>   -0.8373</td> <td>    0.026</td> <td>  -32.269</td> <td> 0.000</td> <td>   -0.888</td> <td>   -0.786</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98058</th>         <td>   -0.5853</td> <td>    0.021</td> <td>  -28.108</td> <td> 0.000</td> <td>   -0.626</td> <td>   -0.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98031</th>         <td>   -0.7771</td> <td>    0.025</td> <td>  -30.699</td> <td> 0.000</td> <td>   -0.827</td> <td>   -0.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98168</th>         <td>   -0.7292</td> <td>    0.026</td> <td>  -28.288</td> <td> 0.000</td> <td>   -0.780</td> <td>   -0.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98002</th>         <td>   -0.9461</td> <td>    0.029</td> <td>  -32.676</td> <td> 0.000</td> <td>   -1.003</td> <td>   -0.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98198</th>         <td>   -0.7810</td> <td>    0.025</td> <td>  -30.890</td> <td> 0.000</td> <td>   -0.831</td> <td>   -0.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98055</th>         <td>   -0.6196</td> <td>    0.026</td> <td>  -24.238</td> <td> 0.000</td> <td>   -0.670</td> <td>   -0.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98004</th>         <td>    1.4165</td> <td>    0.029</td> <td>   49.130</td> <td> 0.000</td> <td>    1.360</td> <td>    1.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98022</th>         <td>   -0.8945</td> <td>    0.030</td> <td>  -29.636</td> <td> 0.000</td> <td>   -0.954</td> <td>   -0.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98178</th>         <td>   -0.5760</td> <td>    0.026</td> <td>  -22.230</td> <td> 0.000</td> <td>   -0.627</td> <td>   -0.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98032</th>         <td>   -0.9667</td> <td>    0.035</td> <td>  -27.250</td> <td> 0.000</td> <td>   -1.036</td> <td>   -0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98059</th>         <td>   -0.2393</td> <td>    0.021</td> <td>  -11.279</td> <td> 0.000</td> <td>   -0.281</td> <td>   -0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98103</th>         <td>    1.0297</td> <td>    0.022</td> <td>   47.343</td> <td> 0.000</td> <td>    0.987</td> <td>    1.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98117</th>         <td>    0.9331</td> <td>    0.021</td> <td>   45.326</td> <td> 0.000</td> <td>    0.893</td> <td>    0.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98115</th>         <td>    0.8978</td> <td>    0.020</td> <td>   45.107</td> <td> 0.000</td> <td>    0.859</td> <td>    0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98112</th>         <td>    1.2853</td> <td>    0.030</td> <td>   43.328</td> <td> 0.000</td> <td>    1.227</td> <td>    1.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98188</th>         <td>   -0.7009</td> <td>    0.035</td> <td>  -20.212</td> <td> 0.000</td> <td>   -0.769</td> <td>   -0.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98199</th>         <td>    0.9655</td> <td>    0.025</td> <td>   38.322</td> <td> 0.000</td> <td>    0.916</td> <td>    1.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98105</th>         <td>    1.1546</td> <td>    0.030</td> <td>   39.071</td> <td> 0.000</td> <td>    1.097</td> <td>    1.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98119</th>         <td>    1.2412</td> <td>    0.034</td> <td>   37.018</td> <td> 0.000</td> <td>    1.175</td> <td>    1.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98040</th>         <td>    0.9748</td> <td>    0.029</td> <td>   33.414</td> <td> 0.000</td> <td>    0.918</td> <td>    1.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_4</th>            <td>    0.6018</td> <td>    0.035</td> <td>   17.396</td> <td> 0.000</td> <td>    0.534</td> <td>    0.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98033</th>         <td>    0.7407</td> <td>    0.022</td> <td>   33.917</td> <td> 0.000</td> <td>    0.698</td> <td>    0.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98107</th>         <td>    0.9939</td> <td>    0.029</td> <td>   33.784</td> <td> 0.000</td> <td>    0.936</td> <td>    1.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98116</th>         <td>    0.7997</td> <td>    0.025</td> <td>   32.010</td> <td> 0.000</td> <td>    0.751</td> <td>    0.849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98122</th>         <td>    0.8676</td> <td>    0.027</td> <td>   32.597</td> <td> 0.000</td> <td>    0.815</td> <td>    0.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98109</th>         <td>    1.2534</td> <td>    0.042</td> <td>   29.997</td> <td> 0.000</td> <td>    1.171</td> <td>    1.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th>   <td>    0.1929</td> <td>    0.011</td> <td>   17.987</td> <td> 0.000</td> <td>    0.172</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_9</th>              <td>    0.2360</td> <td>    0.010</td> <td>   23.052</td> <td> 0.000</td> <td>    0.216</td> <td>    0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_10</th>             <td>    0.3771</td> <td>    0.017</td> <td>   22.592</td> <td> 0.000</td> <td>    0.344</td> <td>    0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>       <td>    0.2880</td> <td>    0.031</td> <td>    9.150</td> <td> 0.000</td> <td>    0.226</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98056</th>         <td>   -0.2380</td> <td>    0.022</td> <td>  -10.939</td> <td> 0.000</td> <td>   -0.281</td> <td>   -0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_7</th>              <td>   -0.1823</td> <td>    0.008</td> <td>  -23.306</td> <td> 0.000</td> <td>   -0.198</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98102</th>         <td>    1.2013</td> <td>    0.044</td> <td>   27.222</td> <td> 0.000</td> <td>    1.115</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98146</th>         <td>   -0.2913</td> <td>    0.025</td> <td>  -11.619</td> <td> 0.000</td> <td>   -0.340</td> <td>   -0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98136</th>         <td>    0.5993</td> <td>    0.027</td> <td>   22.140</td> <td> 0.000</td> <td>    0.546</td> <td>    0.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_3</th>            <td>    0.3808</td> <td>    0.021</td> <td>   17.737</td> <td> 0.000</td> <td>    0.339</td> <td>    0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_2</th>            <td>    0.2412</td> <td>    0.015</td> <td>   16.473</td> <td> 0.000</td> <td>    0.213</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98166</th>         <td>   -0.2345</td> <td>    0.026</td> <td>   -8.868</td> <td> 0.000</td> <td>   -0.286</td> <td>   -0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98039</th>         <td>    1.7839</td> <td>    0.100</td> <td>   17.829</td> <td> 0.000</td> <td>    1.588</td> <td>    1.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98005</th>         <td>    0.6900</td> <td>    0.033</td> <td>   21.226</td> <td> 0.000</td> <td>    0.626</td> <td>    0.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98106</th>         <td>   -0.1046</td> <td>    0.024</td> <td>   -4.335</td> <td> 0.000</td> <td>   -0.152</td> <td>   -0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98148</th>         <td>   -0.5889</td> <td>    0.053</td> <td>  -11.175</td> <td> 0.000</td> <td>   -0.692</td> <td>   -0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98019</th>         <td>   -0.2525</td> <td>    0.031</td> <td>   -8.019</td> <td> 0.000</td> <td>   -0.314</td> <td>   -0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98045</th>         <td>   -0.2230</td> <td>    0.030</td> <td>   -7.476</td> <td> 0.000</td> <td>   -0.281</td> <td>   -0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>      <td>    0.1018</td> <td>    0.006</td> <td>   15.809</td> <td> 0.000</td> <td>    0.089</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98144</th>         <td>    0.5692</td> <td>    0.025</td> <td>   23.105</td> <td> 0.000</td> <td>    0.521</td> <td>    0.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98010</th>         <td>   -0.4391</td> <td>    0.044</td> <td>   -9.900</td> <td> 0.000</td> <td>   -0.526</td> <td>   -0.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>       <td>    0.5299</td> <td>    0.033</td> <td>   16.201</td> <td> 0.000</td> <td>    0.466</td> <td>    0.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98052</th>         <td>    0.4591</td> <td>    0.019</td> <td>   23.717</td> <td> 0.000</td> <td>    0.421</td> <td>    0.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98006</th>         <td>    0.4735</td> <td>    0.022</td> <td>   21.734</td> <td> 0.000</td> <td>    0.431</td> <td>    0.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>        <td>    0.8368</td> <td>    0.059</td> <td>   14.131</td> <td> 0.000</td> <td>    0.721</td> <td>    0.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98008</th>         <td>    0.4823</td> <td>    0.025</td> <td>   19.093</td> <td> 0.000</td> <td>    0.433</td> <td>    0.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>       <td>    0.3816</td> <td>    0.032</td> <td>   12.054</td> <td> 0.000</td> <td>    0.320</td> <td>    0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>renovated</th>         <td>    0.1629</td> <td>    0.016</td> <td>   10.392</td> <td> 0.000</td> <td>    0.132</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98053</th>         <td>    0.3904</td> <td>    0.024</td> <td>   16.300</td> <td> 0.000</td> <td>    0.343</td> <td>    0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view_1</th>            <td>    0.2306</td> <td>    0.023</td> <td>    9.924</td> <td> 0.000</td> <td>    0.185</td> <td>    0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98007</th>         <td>    0.4895</td> <td>    0.034</td> <td>   14.351</td> <td> 0.000</td> <td>    0.423</td> <td>    0.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98029</th>         <td>    0.3752</td> <td>    0.025</td> <td>   15.232</td> <td> 0.000</td> <td>    0.327</td> <td>    0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98125</th>         <td>    0.3534</td> <td>    0.022</td> <td>   15.756</td> <td> 0.000</td> <td>    0.309</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98126</th>         <td>    0.3405</td> <td>    0.024</td> <td>   14.383</td> <td> 0.000</td> <td>    0.294</td> <td>    0.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98177</th>         <td>    0.3600</td> <td>    0.027</td> <td>   13.521</td> <td> 0.000</td> <td>    0.308</td> <td>    0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98075</th>         <td>    0.3139</td> <td>    0.025</td> <td>   12.388</td> <td> 0.000</td> <td>    0.264</td> <td>    0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98074</th>         <td>    0.2813</td> <td>    0.022</td> <td>   12.714</td> <td> 0.000</td> <td>    0.238</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98034</th>         <td>    0.2279</td> <td>    0.020</td> <td>   11.650</td> <td> 0.000</td> <td>    0.190</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98027</th>         <td>    0.2518</td> <td>    0.024</td> <td>   10.397</td> <td> 0.000</td> <td>    0.204</td> <td>    0.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms_log</th>     <td>    0.0360</td> <td>    0.005</td> <td>    7.982</td> <td> 0.000</td> <td>    0.027</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98070</th>         <td>   -0.2999</td> <td>    0.049</td> <td>   -6.068</td> <td> 0.000</td> <td>   -0.397</td> <td>   -0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98014</th>         <td>   -0.2405</td> <td>    0.045</td> <td>   -5.381</td> <td> 0.000</td> <td>   -0.328</td> <td>   -0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_6</th>              <td>   -0.3305</td> <td>    0.013</td> <td>  -25.223</td> <td> 0.000</td> <td>   -0.356</td> <td>   -0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gr_5</th>              <td>   -0.4922</td> <td>    0.028</td> <td>  -17.398</td> <td> 0.000</td> <td>   -0.548</td> <td>   -0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98118</th>         <td>    0.1330</td> <td>    0.020</td> <td>    6.494</td> <td> 0.000</td> <td>    0.093</td> <td>    0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98133</th>         <td>    0.1121</td> <td>    0.021</td> <td>    5.377</td> <td> 0.000</td> <td>    0.071</td> <td>    0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot15_log</th>    <td>   -0.0302</td> <td>    0.007</td> <td>   -4.612</td> <td> 0.000</td> <td>   -0.043</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98072</th>         <td>    0.0938</td> <td>    0.027</td> <td>    3.526</td> <td> 0.000</td> <td>    0.042</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_2</th>         <td>    0.0295</td> <td>    0.010</td> <td>    3.091</td> <td> 0.002</td> <td>    0.011</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedroom_5</th>         <td>   -0.0343</td> <td>    0.011</td> <td>   -2.989</td> <td> 0.003</td> <td>   -0.057</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zip_98108</th>         <td>   -0.0887</td> <td>    0.030</td> <td>   -2.955</td> <td> 0.003</td> <td>   -0.147</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1508.625</td> <th>  Durbin-Watson:     </th> <td>   1.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6158.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.317</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.737</td>  <th>  Cond. No.          </th> <td>    69.3</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              price_log   R-squared:                       0.862\n",
       "Model:                            OLS   Adj. R-squared:                  0.862\n",
       "Method:                 Least Squares   F-statistic:                     1343.\n",
       "Date:                Sat, 20 Jun 2020   Prob (F-statistic):               0.00\n",
       "Time:                        19:20:58   Log-Likelihood:                -8005.9\n",
       "No. Observations:               18735   AIC:                         1.619e+04\n",
       "Df Residuals:                   18647   BIC:                         1.688e+04\n",
       "Df Model:                          87                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "const                -0.3888      0.034    -11.422      0.000      -0.455      -0.322\n",
       "basement              0.0613      0.012      5.045      0.000       0.037       0.085\n",
       "sqft_above_log        0.1453      0.011     13.770      0.000       0.125       0.166\n",
       "sqft_living15_log     0.1017      0.005     22.147      0.000       0.093       0.111\n",
       "zip_98023            -0.9883      0.020    -49.324      0.000      -1.028      -0.949\n",
       "zip_98042            -0.8150      0.019    -41.807      0.000      -0.853      -0.777\n",
       "zip_98001            -0.9439      0.023    -41.833      0.000      -0.988      -0.900\n",
       "zip_98038            -0.5985      0.020    -30.539      0.000      -0.637      -0.560\n",
       "zip_98092            -0.9328      0.024    -38.584      0.000      -0.980      -0.885\n",
       "zip_98003            -0.8862      0.025    -35.310      0.000      -0.935      -0.837\n",
       "zip_98030            -0.8373      0.026    -32.269      0.000      -0.888      -0.786\n",
       "zip_98058            -0.5853      0.021    -28.108      0.000      -0.626      -0.545\n",
       "zip_98031            -0.7771      0.025    -30.699      0.000      -0.827      -0.728\n",
       "zip_98168            -0.7292      0.026    -28.288      0.000      -0.780      -0.679\n",
       "zip_98002            -0.9461      0.029    -32.676      0.000      -1.003      -0.889\n",
       "zip_98198            -0.7810      0.025    -30.890      0.000      -0.831      -0.731\n",
       "zip_98055            -0.6196      0.026    -24.238      0.000      -0.670      -0.569\n",
       "zip_98004             1.4165      0.029     49.130      0.000       1.360       1.473\n",
       "zip_98022            -0.8945      0.030    -29.636      0.000      -0.954      -0.835\n",
       "zip_98178            -0.5760      0.026    -22.230      0.000      -0.627      -0.525\n",
       "zip_98032            -0.9667      0.035    -27.250      0.000      -1.036      -0.897\n",
       "zip_98059            -0.2393      0.021    -11.279      0.000      -0.281      -0.198\n",
       "zip_98103             1.0297      0.022     47.343      0.000       0.987       1.072\n",
       "zip_98117             0.9331      0.021     45.326      0.000       0.893       0.973\n",
       "zip_98115             0.8978      0.020     45.107      0.000       0.859       0.937\n",
       "zip_98112             1.2853      0.030     43.328      0.000       1.227       1.343\n",
       "zip_98188            -0.7009      0.035    -20.212      0.000      -0.769      -0.633\n",
       "zip_98199             0.9655      0.025     38.322      0.000       0.916       1.015\n",
       "zip_98105             1.1546      0.030     39.071      0.000       1.097       1.212\n",
       "zip_98119             1.2412      0.034     37.018      0.000       1.175       1.307\n",
       "zip_98040             0.9748      0.029     33.414      0.000       0.918       1.032\n",
       "view_4                0.6018      0.035     17.396      0.000       0.534       0.670\n",
       "zip_98033             0.7407      0.022     33.917      0.000       0.698       0.783\n",
       "zip_98107             0.9939      0.029     33.784      0.000       0.936       1.052\n",
       "zip_98116             0.7997      0.025     32.010      0.000       0.751       0.849\n",
       "zip_98122             0.8676      0.027     32.597      0.000       0.815       0.920\n",
       "zip_98109             1.2534      0.042     29.997      0.000       1.171       1.335\n",
       "sqft_living_log       0.1929      0.011     17.987      0.000       0.172       0.214\n",
       "gr_9                  0.2360      0.010     23.052      0.000       0.216       0.256\n",
       "gr_10                 0.3771      0.017     22.592      0.000       0.344       0.410\n",
       "condition_3           0.2880      0.031      9.150      0.000       0.226       0.350\n",
       "zip_98056            -0.2380      0.022    -10.939      0.000      -0.281      -0.195\n",
       "gr_7                 -0.1823      0.008    -23.306      0.000      -0.198      -0.167\n",
       "zip_98102             1.2013      0.044     27.222      0.000       1.115       1.288\n",
       "zip_98146            -0.2913      0.025    -11.619      0.000      -0.340      -0.242\n",
       "zip_98136             0.5993      0.027     22.140      0.000       0.546       0.652\n",
       "view_3                0.3808      0.021     17.737      0.000       0.339       0.423\n",
       "view_2                0.2412      0.015     16.473      0.000       0.213       0.270\n",
       "zip_98166            -0.2345      0.026     -8.868      0.000      -0.286      -0.183\n",
       "zip_98039             1.7839      0.100     17.829      0.000       1.588       1.980\n",
       "zip_98005             0.6900      0.033     21.226      0.000       0.626       0.754\n",
       "zip_98106            -0.1046      0.024     -4.335      0.000      -0.152      -0.057\n",
       "zip_98148            -0.5889      0.053    -11.175      0.000      -0.692      -0.486\n",
       "zip_98019            -0.2525      0.031     -8.019      0.000      -0.314      -0.191\n",
       "zip_98045            -0.2230      0.030     -7.476      0.000      -0.281      -0.165\n",
       "sqft_lot_log          0.1018      0.006     15.809      0.000       0.089       0.114\n",
       "zip_98144             0.5692      0.025     23.105      0.000       0.521       0.617\n",
       "zip_98010            -0.4391      0.044     -9.900      0.000      -0.526      -0.352\n",
       "condition_5           0.5299      0.033     16.201      0.000       0.466       0.594\n",
       "zip_98052             0.4591      0.019     23.717      0.000       0.421       0.497\n",
       "zip_98006             0.4735      0.022     21.734      0.000       0.431       0.516\n",
       "waterfront            0.8368      0.059     14.131      0.000       0.721       0.953\n",
       "zip_98008             0.4823      0.025     19.093      0.000       0.433       0.532\n",
       "condition_4           0.3816      0.032     12.054      0.000       0.320       0.444\n",
       "renovated             0.1629      0.016     10.392      0.000       0.132       0.194\n",
       "zip_98053             0.3904      0.024     16.300      0.000       0.343       0.437\n",
       "view_1                0.2306      0.023      9.924      0.000       0.185       0.276\n",
       "zip_98007             0.4895      0.034     14.351      0.000       0.423       0.556\n",
       "zip_98029             0.3752      0.025     15.232      0.000       0.327       0.423\n",
       "zip_98125             0.3534      0.022     15.756      0.000       0.309       0.397\n",
       "zip_98126             0.3405      0.024     14.383      0.000       0.294       0.387\n",
       "zip_98177             0.3600      0.027     13.521      0.000       0.308       0.412\n",
       "zip_98075             0.3139      0.025     12.388      0.000       0.264       0.364\n",
       "zip_98074             0.2813      0.022     12.714      0.000       0.238       0.325\n",
       "zip_98034             0.2279      0.020     11.650      0.000       0.190       0.266\n",
       "zip_98027             0.2518      0.024     10.397      0.000       0.204       0.299\n",
       "bathrooms_log         0.0360      0.005      7.982      0.000       0.027       0.045\n",
       "zip_98070            -0.2999      0.049     -6.068      0.000      -0.397      -0.203\n",
       "zip_98014            -0.2405      0.045     -5.381      0.000      -0.328      -0.153\n",
       "gr_6                 -0.3305      0.013    -25.223      0.000      -0.356      -0.305\n",
       "gr_5                 -0.4922      0.028    -17.398      0.000      -0.548      -0.437\n",
       "zip_98118             0.1330      0.020      6.494      0.000       0.093       0.173\n",
       "zip_98133             0.1121      0.021      5.377      0.000       0.071       0.153\n",
       "sqft_lot15_log       -0.0302      0.007     -4.612      0.000      -0.043      -0.017\n",
       "zip_98072             0.0938      0.027      3.526      0.000       0.042       0.146\n",
       "bedroom_2             0.0295      0.010      3.091      0.002       0.011       0.048\n",
       "bedroom_5            -0.0343      0.011     -2.989      0.003      -0.057      -0.012\n",
       "zip_98108            -0.0887      0.030     -2.955      0.003      -0.147      -0.030\n",
       "==============================================================================\n",
       "Omnibus:                     1508.625   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6158.887\n",
       "Skew:                          -0.317   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.737   Cond. No.                         69.3\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fin = X[result]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model = sm.OLS(y,X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our R-squared dropped from 0.863 to 0.862."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model Validation\n",
    "# Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train_test_split with sklearn.model_selection\n",
    "# Split the data into training and test sets (assign 20% to test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14988 3747 14988 3747\n"
     ]
    }
   ],
   "source": [
    "# A brief preview of train-test split\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the residuals and calculate the MSE for training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Mean Squarred Error: 0.13665599109778936\n",
      "Test Mean Squarred Error: 0.1416064813008251\n"
     ]
    }
   ],
   "source": [
    "# Find mean_squared_error with sklearn.metrics\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "print('Train Mean Squarred Error:', train_mse)\n",
    "print('Test Mean Squarred Error:', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a big difference between the train and test MSE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform 10-fold cross-validation to get the mean squared error through scikit-learn. Let's have a look at the six individual MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Find cross_val_score with klearn.model_selection\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "cv_10_results = cross_val_score(linreg, X, y, cv=10, scoring=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13982918464853034"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_10_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train mean squared error is 0.138 and our mean squared error is 0.140. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the effect of train-test split size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffde42d6668>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(11)\n",
    "\n",
    "train_err = []\n",
    "test_err = []\n",
    "t_sizes = list(range(5,100,5))\n",
    "for t_size in t_sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_size/100)\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_hat_train = linreg.predict(X_train)\n",
    "    y_hat_test = linreg.predict(X_test)\n",
    "    train_err.append(mean_squared_error(y_train, y_hat_train))\n",
    "    test_err.append(mean_squared_error(y_test, y_hat_test))\n",
    "plt.scatter(t_sizes, train_err, label='Training Error')\n",
    "plt.scatter(t_sizes, test_err, label='Testing Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed.drop('price_log', axis=1)\n",
    "y = preprocessed['price_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same random state to compare - Only need one set of training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-cadab444d9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlinreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv_10_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Compare with old scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcv_10_results\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcv_10_results_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 231\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 463\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n\u001b[0;32m--> 722\u001b[0;31m                         dtype=None)\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "cv_10_results = np.mean(cross_val_score(linreg, X_train, np.log(y_train), cv=10, scoring=\"r2\"))\n",
    "# Compare with old scores\n",
    "cv_10_results/cv_10_results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linreg.fit(X_train, np.log(y_train))\n",
    "y_hat_train = np.exp(linreg.predict(X_train))\n",
    "y_hat_test = np.exp(linreg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":\n",
    "# Selected Features True Data Scores:\n",
    "print(r2_score(y_train, y_hat_train))\n",
    "print(r2_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":\n",
    "# Compare to old scores\n",
    "print(r2_score(y_train, y_hat_train) - r2_score(y1_train, y1_hat_train))\n",
    "print(r2_score(y_test, y_hat_test) - r2_score(y1_test, y1_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare test scores:\n",
    "print('-- Performance Increase from Raw Model to Final Model --\\n')\n",
    "print(\"Raw increase in R2:\", r2_score(y_test, y_hat_test) - r2_score(y2_test, y2_hat_test))\n",
    "print(\"Percentage improvement:\", 100*(r2_score(y_test, y_hat_test) / r2_score(y2_test, y2_hat_test) - 1), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predicted vs. actual test price\n",
    "vis = y_test.copy()\n",
    "vis['predicted'] = np.round(y_hat_test)\n",
    "vis['price - predicted'] = np.round(vis['price'] - vis['predicted'])\n",
    "vis['price / predicted'] = np.round(vis['price'] / vis['predicted'], 3)\n",
    "vis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predicted vs. actual test price\n",
    "vis = y_test.copy()\n",
    "vis['predicted'] = np.round(y_hat_test)\n",
    "vis['price - predicted'] = np.round(vis['price'] - vis['predicted'])\n",
    "vis['price / predicted'] = np.round(vis['price'] / vis['predicted'], 3)\n",
    "vis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = y_test.copy()\n",
    "vis['predicted'] = np.round(y_hat_test)\n",
    "vis['price - predicted'] = np.round(vis['price'] - vis['predicted'])\n",
    "vis['price / predicted'] = np.round(vis['price'] / vis['predicted'], 3)\n",
    "vis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a final polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domgross is the outcome variable\n",
    "X = df[['budget', 'imdbRating', 'Metascore', 'imdbVotes']]\n",
    "y = df['domgross']\n",
    "\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the test set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set predictions\n",
    "lm_train_predictions = linreg.predict(X_train_scaled)\n",
    "\n",
    "# Test set predictions \n",
    "lm_test_predictions = linreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Run this cell - vertical distance between the points and the line denote the errors\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_train, lm_train_predictions, label='Model')\n",
    "plt.plot(y_train, y_train, label='Actual data')\n",
    "plt.title('Model vs data for training set')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell - vertical distance between the points and the line denote the errors\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, lm_test_predictions, label='Model')\n",
    "plt.plot(y_test, y_test, label='Actual data')\n",
    "plt.title('Model vs data for test set')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
